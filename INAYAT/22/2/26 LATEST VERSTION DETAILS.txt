

# Product Requirements Document (PRD)
## Shield-Ryzen V2 — Real-Time Deepfake Detection System
### AMD Slingshot 2026

---

**Document Version:** 1.0  
**Author:** Inayat Hussain  
**Date:** July 2025  
**Status:** Prototype (Vibe Coding)  
**Classification:** Competition Submission

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Problem Statement](#2-problem-statement)
3. [Product Vision & Objectives](#3-product-vision--objectives)
4. [Target Users & Personas](#4-target-users--personas)
5. [Scope & Boundaries](#5-scope--boundaries)
6. [Functional Requirements](#6-functional-requirements)
7. [Non-Functional Requirements](#7-non-functional-requirements)
8. [System Architecture](#8-system-architecture)
9. [Core Detection Pipeline](#9-core-detection-pipeline)
10. [Neural Network Specification](#10-neural-network-specification)
11. [Face Detection & Preprocessing](#11-face-detection--preprocessing)
12. [3-Tier Security Decision Engine](#12-3-tier-security-decision-engine)
13. [Plugin Extensibility Framework](#13-plugin-extensibility-framework)
14. [User Interface & HUD](#14-user-interface--hud)
15. [Security, Privacy & Compliance](#15-security-privacy--compliance)
16. [Configuration & Deployment](#16-configuration--deployment)
17. [Performance Requirements](#17-performance-requirements)
18. [Dependencies & Technology Stack](#18-dependencies--technology-stack)
19. [Known Limitations & Constraints](#19-known-limitations--constraints)
20. [Future Roadmap](#20-future-roadmap)
21. [Success Metrics](#21-success-metrics)
22. [Glossary](#22-glossary)

---

## 1. Executive Summary

**Shield-Ryzen V2** (codename: Shield-Xception) is a real-time, fully local deepfake detection system designed for the **AMD Slingshot 2026** competition. The system captures live webcam video, detects faces, and classifies each face as **REAL** or **FAKE** using a multi-layered security approach combining deep neural network inference, biometric liveness verification, and digital forensic analysis.

The prototype runs 100% offline with zero cloud dependency, ensuring complete user privacy. It is developed on an NVIDIA RTX 3050 Laptop GPU and architecturally targeted for deployment on **AMD Ryzen AI NPUs** via ONNX Runtime.

### Key Differentiators

| Differentiator | Description |
|---|---|
| **3-Tier Decision Fusion** | Neural network + liveness detection + forensic analysis combined via a deterministic truth table |
| **100% Local Inference** | No internet, no cloud, no telemetry — fully offline |
| **Plugin Architecture** | 9 modular analysis plugins (heartbeat, adversarial patch, lip sync, etc.) |
| **Adaptive Calibration** | Dynamic baseline scaling that adapts to each user's unique biometrics |
| **AMD NPU-Ready** | ONNX INT8 model exportable for Ryzen AI XDNA acceleration |
| **Privacy-First Design** | AES-256-GCM encrypted biometric data, ephemeral keys, no disk persistence |

---

## 2. Problem Statement

### 2.1 The Threat Landscape

Deepfake technology has become commoditized. Tools like DeepFaceLab, FaceSwap, and emerging diffusion-based face generators enable anyone to create convincing face manipulations in real time. The threats include:

- **Identity fraud** in video KYC and remote authentication
- **Impersonation** in video conferences and remote work
- **Social engineering** using fabricated video evidence
- **Presentation attacks** (screen replay, printed photos, 3D masks) against biometric systems

### 2.2 Gap in Current Solutions

| Current Approach | Limitation |
|---|---|
| Cloud-based deepfake APIs | Requires internet; privacy concerns; latency |
| Browser extensions | Limited to specific platforms; no biometric verification |
| Forensic tools (manual) | Not real-time; require expert operation |
| Hardware security keys | Don't address video-based impersonation |
| Single-model detectors | Vulnerable to adversarial attacks; no liveness checks |

### 2.3 Core Problem Statement

> There is no widely available, real-time, privacy-preserving deepfake detection system that combines neural classification with biometric liveness verification and runs entirely on consumer-grade edge hardware.

Shield-Ryzen V2 addresses this gap.

---

## 3. Product Vision & Objectives

### 3.1 Vision

> Make real-time deepfake detection accessible on every AMD-powered device, protecting users' digital identity without compromising their privacy.

### 3.2 Objectives

| ID | Objective | Measurable Target |
|---|---|---|
| **O1** | Detect deepfakes in live video with high accuracy | ≥85% detection rate on FaceForensics++ c23 test set |
| **O2** | Verify human liveness through blink detection | Blink detected within 10-second observation window |
| **O3** | Detect screen replay / presentation attacks | ≥80% screen detection via forensic pipeline |
| **O4** | Run entirely on local hardware | Zero network calls; 100% offline operation |
| **O5** | Achieve real-time performance | ≥15 FPS on GPU; ≥8 FPS on CPU |
| **O6** | Protect user biometric data | AES-256-GCM encryption; no disk persistence |
| **O7** | Target AMD Ryzen AI NPU deployment | ONNX INT8 model exported and validated |
| **O8** | Provide extensible analysis | Plugin system with ≥9 analysis modules |

---

## 4. Target Users & Personas

### 4.1 Primary Persona: Security-Conscious Professional

| Attribute | Detail |
|---|---|
| **Name** | Alex — Remote Worker |
| **Context** | Participates in daily video calls; handles sensitive business information |
| **Pain Point** | Cannot verify if the person on the other side of a video call is real |
| **Need** | A local tool that flags deepfake impersonation attempts in real time |
| **Tech Level** | Intermediate — can install Python packages and run CLI commands |

### 4.2 Secondary Persona: Identity Verification Engineer

| Attribute | Detail |
|---|---|
| **Name** | Priya — KYC Developer |
| **Context** | Building remote identity verification into a fintech application |
| **Pain Point** | Cloud-based liveness APIs are expensive, slow, and raise GDPR concerns |
| **Need** | An embeddable local detection engine with multi-layered verification |
| **Tech Level** | Advanced — comfortable with Python, ONNX, model deployment |

### 4.3 Tertiary Persona: Competition Judge / Evaluator

| Attribute | Detail |
|---|---|
| **Name** | AMD Slingshot 2026 Evaluator |
| **Context** | Assessing prototype quality, innovation, and AMD hardware utilization |
| **Pain Point** | Needs to understand architecture depth and AMD alignment |
| **Need** | Clear documentation, working demo, and NPU deployment path |
| **Tech Level** | Expert |

---

## 5. Scope & Boundaries

### 5.1 In Scope (This Prototype)

| Feature | Status |
|---|---|
| Real-time webcam deepfake detection | ✅ Implemented |
| XceptionNet neural classification | ✅ Implemented |
| Blink-based liveness detection | ✅ Implemented |
| 5-layer forensic screen detection | ✅ Implemented |
| 3-tier decision fusion with hysteresis | ✅ Implemented |
| 9 analysis plugins | ✅ Implemented |
| AES-256-GCM biometric encryption | ✅ Implemented |
| ONNX model export for AMD NPU | ✅ Implemented |
| Glassmorphism HUD display | ✅ Implemented |
| JSONL audit logging | ✅ Implemented |
| CLI launcher with configuration | ✅ Implemented |

### 5.2 Out of Scope (Not in This Prototype)

| Feature | Reason |
|---|---|
| Production AMD NPU deployment | Requires Vitis AI EP hardware testing |
| Audio deepfake detection | Separate problem domain |
| Face recognition / identity matching | Privacy scope — detection only, not identification |
| Mobile deployment (Android/iOS) | Desktop-first prototype |
| Web interface / API server | CLI/desktop application only |
| Real-time training / fine-tuning | Inference-only system |
| Multi-camera production stereo | Single-camera prototype (plugin skeleton exists) |
| Diffusion-model deepfake training data | XceptionNet trained on FaceForensics++ only |

---

## 6. Functional Requirements

### FR1: Video Capture

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR1.1 | System SHALL capture frames from a USB/built-in webcam | P0 | Frame captured within 100ms of startup |
| FR1.2 | System SHALL support multiple camera indices via `--source` flag | P1 | Cameras 0–4 selectable |
| FR1.3 | System SHALL validate frame quality (non-black, non-corrupt) | P0 | Black/corrupt frames rejected with fallback |
| FR1.4 | System SHALL use DirectShow backend on Windows for compatibility | P2 | `cv2.CAP_DSHOW` used when available |
| FR1.5 | System SHALL monitor camera health (FPS, drop rate) | P1 | Health metrics available in HUD |

### FR2: Face Detection

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR2.1 | System SHALL detect up to 2 faces per frame | P0 | Multi-face processing verified |
| FR2.2 | System SHALL extract 478-point face mesh landmarks | P0 | MediaPipe FaceLandmarker with blendshapes |
| FR2.3 | System SHALL convert 478 landmarks to 68-point standard | P0 | Mapping validated against Dlib standard |
| FR2.4 | System SHALL estimate head pose (yaw, pitch, roll) | P1 | Pose computed via `cv2.solvePnP()` |
| FR2.5 | System SHALL compute occlusion score per face | P2 | Depth variance of key landmarks measured |
| FR2.6 | System SHALL fall back to DNN SSD if MediaPipe unavailable | P1 | Fallback detector loads and runs |
| FR2.7 | System SHALL track faces across frames via IoU matching | P1 | Persistent face IDs assigned |

### FR3: Neural Deepfake Classification

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR3.1 | System SHALL classify each face as REAL or FAKE using XceptionNet | P0 | Output: `[fake_prob, real_prob]` via softmax |
| FR3.2 | System SHALL normalize face crops to `[-1, +1]` range at 299×299 | P0 | FF++ normalization: `(x/255 - 0.5) / 0.5` |
| FR3.3 | System SHALL apply temperature scaling (T=1.5) for calibration | P0 | Overconfident outputs smoothed |
| FR3.4 | System SHALL prefer ONNX model, fall back to PyTorch | P1 | ONNX loads first; `.pth` as backup |
| FR3.5 | System SHALL verify model integrity via SHA-256 hash | P0 | `ModelTamperingError` on mismatch |

### FR4: Liveness Detection

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR4.1 | System SHALL compute Eye Aspect Ratio (EAR) per frame | P0 | Formula: `(|p2-p6|+|p3-p5|) / (2×|p1-p4|)` |
| FR4.2 | System SHALL apply cosine yaw compensation to EAR | P1 | `EAR / cos(yaw)` when `|yaw| > 20°` |
| FR4.3 | System SHALL use Dynamic Baseline Scaling for thresholds | P0 | Per-user adaptive open/close thresholds |
| FR4.4 | System SHALL validate blink duration (60ms–500ms) | P0 | Noise and long-close rejected |
| FR4.5 | System SHALL prefer blendshape blink detection over EAR | P1 | Blendshape indices 9+10 averaged > 0.5 |
| FR4.6 | System SHALL analyze blink pattern regularity | P2 | CV > 0.3 = natural, < 0.3 = robotic |
| FR4.7 | System SHALL flag no-blink for >10 seconds as suspicious | P1 | Timer-based alert |

### FR5: Forensic Analysis

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR5.1 | System SHALL compute Laplacian variance for texture sharpness | P0 | Threshold: baseline × 0.4 or default 15.0 |
| FR5.2 | System SHALL cross-validate texture against distance physics | P1 | Inverse-square model with 8× safety margin |
| FR5.3 | System SHALL detect Moiré patterns via FFT radial profile | P1 | Autocorrelation score > 0.70 = screen |
| FR5.4 | System SHALL detect screen light emission (brightness uniformity, chrominance, blue bias) | P1 | Weighted fusion: 0.45 + 0.35 + 0.20 |
| FR5.5 | System SHALL require physical evidence for screen confirmation | P0 | Physics-only signals NEVER confirm alone |
| FR5.6 | System SHALL fuse weak signals (2+ signals > 0.35) | P1 | Combined > 0.45 with physical evidence confirms |

### FR6: Decision Fusion

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR6.1 | System SHALL combine T1, T2, T3 via deterministic truth table | P0 | 8-row truth table implemented |
| FR6.2 | System SHALL apply hysteresis: immediate escalation, 5-frame de-escalation | P0 | No flickering between states |
| FR6.3 | System SHALL support 7 output states (VERIFIED, REAL, WAIT_BLINK, SUSPICIOUS, HIGH_RISK, FAKE, CRITICAL) | P0 | All states reachable |
| FR6.4 | System SHALL promote REAL → VERIFIED when all tiers pass + confidence > 89% | P0 | Promotion logic in engine |
| FR6.5 | System SHALL integrate plugin votes via majority rule | P1 | >50% FAKE votes → plugin consensus FAKE |
| FR6.6 | System SHALL allow strong plugin consensus (75%+) to override neural verdict | P2 | Override condition documented |

### FR7: Display & Interaction

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR7.1 | System SHALL display real-time video with face bounding boxes | P0 | Colored boxes per state |
| FR7.2 | System SHALL show trust verdict badge above each face | P0 | State name + color coded |
| FR7.3 | System SHALL render a metrics dashboard panel | P1 | Neural confidence, EAR, blinks, distance, pose |
| FR7.4 | System SHALL support fullscreen toggle (F key) | P2 | Window ↔ fullscreen |
| FR7.5 | System SHALL support window resizing via `+`/`-` and presets `1`-`4` | P2 | 960×540 to 1920×1080 |
| FR7.6 | System SHALL exit cleanly on `Q` or `ESC` | P0 | All threads stopped, resources freed |

### FR8: Audit & Logging

| ID | Requirement | Priority | Acceptance Criteria |
|---|---|---|---|
| FR8.1 | System SHALL log every frame decision in JSONL format | P1 | Timestamp, face results, FPS, memory |
| FR8.2 | System SHALL use thread-safe file writes | P0 | Mutex-locked append |
| FR8.3 | System SHALL include timing breakdown per frame | P2 | Detection, inference, forensic, total times |

---

## 7. Non-Functional Requirements

### NFR1: Performance

| ID | Requirement | Target |
|---|---|---|
| NFR1.1 | End-to-end latency (capture → display) | ≤150ms (GPU), ≤300ms (CPU) |
| NFR1.2 | Frame rate (GPU: RTX 3050) | ≥15 FPS sustained |
| NFR1.3 | Frame rate (CPU-only) | ≥8 FPS sustained |
| NFR1.4 | Memory footprint (steady state) | ≤1.5 GB RAM |
| NFR1.5 | Memory growth over 1 hour | ≤500 MB (GC triggered) |
| NFR1.6 | Model load time | ≤5 seconds (ONNX), ≤10 seconds (PyTorch) |
| NFR1.7 | Startup-to-first-frame | ≤15 seconds including calibration |

### NFR2: Reliability

| ID | Requirement | Target |
|---|---|---|
| NFR2.1 | Continuous operation | ≥1 hour without crash |
| NFR2.2 | Camera disconnect recovery | Automatic reconnect within 5 seconds |
| NFR2.3 | Model file corruption handling | Graceful error with user message |
| NFR2.4 | Thread crash isolation | Camera/AI thread failures don't crash main |

### NFR3: Security

| ID | Requirement | Target |
|---|---|---|
| NFR3.1 | Biometric data encryption | AES-256-GCM, ephemeral keys |
| NFR3.2 | Model integrity verification | SHA-256 hash check on every load |
| NFR3.3 | Network isolation | Zero outbound connections (verifiable) |
| NFR3.4 | Disk persistence of face data | None — RAM-only, wiped on exit |

### NFR4: Portability

| ID | Requirement | Target |
|---|---|---|
| NFR4.1 | Primary OS support | Windows 10/11 (64-bit) |
| NFR4.2 | Secondary OS support | Linux (Ubuntu 22.04+), macOS (best effort) |
| NFR4.3 | GPU support | NVIDIA CUDA (dev), AMD ROCm (target) |
| NFR4.4 | NPU readiness | ONNX INT8 model exported for Ryzen AI |

### NFR5: Usability

| ID | Requirement | Target |
|---|---|---|
| NFR5.1 | Installation | Single `pip install -r requirements.txt` |
| NFR5.2 | Launch command | Single `python start_shield.py` command |
| NFR5.3 | User feedback | Visual HUD with clear color-coded verdicts |
| NFR5.4 | No training required | System auto-calibrates to user |

---

## 8. System Architecture

### 8.1 High-Level Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Shield-Ryzen V2 System                       │
│                                                                     │
│  ┌──────────────┐   ┌──────────────────────────────┐   ┌─────────┐ │
│  │   Input       │   │      Processing Core         │   │  Output │ │
│  │   Layer       │   │                              │   │  Layer  │ │
│  │              │   │  ┌──────────┐  ┌──────────┐  │   │         │ │
│  │  ShieldCamera │──>│  │Face      │  │Neural    │  │──>│ShieldHUD│ │
│  │  (validated   │   │  │Pipeline  │  │Inference │  │   │(glassmor│ │
│  │   frames)     │   │  │(MediaPipe│  │(Xception)│  │   │phism    │ │
│  │              │   │  │+DNN SSD) │  │          │  │   │overlay) │ │
│  └──────────────┘   │  └──────────┘  └──────────┘  │   │         │ │
│                     │  ┌──────────┐  ┌──────────┐  │   │  Audit  │ │
│                     │  │Liveness  │  │Forensic  │  │   │  Logger │ │
│                     │  │Engine    │  │Engine    │  │   │ (JSONL) │ │
│                     │  │(EAR+DBS) │  │(5-layer) │  │   │         │ │
│                     │  └──────────┘  └──────────┘  │   └─────────┘ │
│                     │  ┌──────────┐  ┌──────────┐  │               │
│                     │  │Plugin    │  │Decision  │  │               │
│                     │  │Framework │  │State     │  │               │
│                     │  │(9 mods)  │  │Machine   │  │               │
│                     │  └──────────┘  └──────────┘  │               │
│                     └──────────────────────────────┘               │
│                                                                     │
│  ┌──────────────────────────────────────────────────────────────┐   │
│  │  Cross-Cutting: Crypto (AES-256) │ Config (YAML) │ Types    │   │
│  └──────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
```

### 8.2 Triple-Buffer Async Pipeline

The system uses a **3-thread architecture** to decouple capture, processing, and display:

```
┌───────────────────┐       ┌───────────────────┐       ┌───────────────────┐
│   CAMERA THREAD   │       │    AI THREAD       │       │   MAIN THREAD     │
│   (Producer)      │       │   (Processor)      │       │   (Consumer)      │
│                   │       │                    │       │                   │
│ • cv2.VideoCapture│       │ • Face detection   │       │ • cv2.imshow()    │
│ • Frame validation│       │ • Neural inference │       │ • HUD rendering   │
│ • Quality checks  │       │ • Liveness analysis│       │ • Keyboard input  │
│ • Health monitor  │       │ • Forensic analysis│       │ • Window mgmt     │
│                   │       │ • Plugin execution │       │                   │
│                   │       │ • State machine    │       │                   │
│                   │       │ • Audit logging    │       │                   │
└────────┬──────────┘       └────────┬──────────┘       └───────────────────┘
         │                           │                           ▲
         │    camera_queue           │    result_queue           │
         │    (maxsize=1)            │    (maxsize=1)            │
         └───────────────────────────┘───────────────────────────┘
```

**Design Rationale:**
- `maxsize=1` queues ensure the system always processes the **most recent frame**, not queued-up stale frames
- Frames are dropped (not queued) when the AI thread is busy — acceptable trade-off for freshness
- Main thread never blocks on AI processing — display remains responsive
- Each thread can run at its own natural rate

### 8.3 File Architecture

```
shield-ryzen-v2/
├── start_shield.py              # CLI launcher (260 lines)
├── shield_engine.py             # Central orchestrator (897 lines)
├── shield_xception.py           # XceptionNet model (512 lines)
├── shield_face_pipeline.py      # Face detection pipeline (900 lines)
├── shield_utils_core.py         # Algorithms & utilities (1269 lines)
├── shield_camera.py             # Camera management (326 lines)
├── shield_hud.py                # HUD rendering (536 lines)
├── shield_crypto.py             # AES-256-GCM encryption (114 lines)
├── shield_logger.py             # JSONL audit logger (108 lines)
├── shield_plugin.py             # Plugin base class (69 lines)
├── shield_types.py              # Data types (32 lines)
├── v3_xdna_engine.py            # AMD XDNA NPU engine (84 lines)
├── export_onnx.py               # ONNX conversion (76 lines)
├── config.yaml                  # Runtime configuration (45 lines)
├── requirements.txt             # Python dependencies
├── plugins/
│   ├── rppg_heartbeat.py        # Heartbeat detection (172 lines)
│   ├── challenge_response.py    # Simon Says liveness (163 lines)
│   ├── stereo_depth.py          # Dual-camera depth (133 lines)
│   ├── skin_reflectance.py      # Skin analysis (119 lines)
│   ├── codec_forensics.py       # Compression detection (118 lines)
│   ├── frequency_analyzer.py    # FFT GAN detection (117 lines)
│   ├── adversarial_detector.py  # Patch detection (113 lines)
│   ├── lip_sync_verifier.py     # Lip-sync check (105 lines)
│   └── arcface_reid.py          # Identity matching (149 lines)
└── models/
    ├── ffpp_c23.pth             # PyTorch weights (83.5 MB)
    ├── shield_ryzen_int8.onnx   # Quantized ONNX (21.4 MB)
    └── face_landmarker_v2_with_blendshapes.task  # MediaPipe (3.75 MB)
```

**Total:** ~5,500 lines of core code + ~1,300 lines of plugins = ~6,800 lines

---

## 9. Core Detection Pipeline

### 9.1 Startup Sequence

```
Step 1: CLI Parsing
    └── start_shield.py parses --cpu, --source, --windowed, --size

Step 2: Engine Initialization
    ├── ShieldCamera(source, backend=DirectShow)
    ├── ShieldFacePipeline(face_landmarker_v2_with_blendshapes.task)
    ├── Model Loading (preference order):
    │   ├── 1st: shield_ryzen_int8.onnx (ONNX Runtime)
    │   └── 2nd: ffpp_c23.pth (PyTorch + CUDA/CPU)
    ├── SHA-256 Model Integrity Check
    ├── ConfidenceCalibrator(temperature=1.5)
    ├── DecisionStateMachine(hysteresis_frames=5) per face
    ├── BlinkTracker per face
    └── Plugin Discovery & Loading (plugins/*.py)

Step 3: Startup Calibration (~100 frames)
    ├── Measure Laplacian variance baseline
    ├── Measure user's natural EAR range
    ├── Assess lighting conditions (GOOD/MODERATE/LOW_LIGHT)
    └── Save to shield_calibration.json

Step 4: Thread Launch
    ├── Camera thread → camera_queue (maxsize=1)
    └── AI thread → result_queue (maxsize=1)

Step 5: Main Loop
    └── get_latest_result() → hud.render() → cv2.imshow()
```

### 9.2 Per-Frame Processing Pipeline

```
Frame Arrival (from camera_queue)
│
├── STAGE 1: Face Detection (ShieldFacePipeline)
│   ├── MediaPipe FaceLandmarker → 478 landmarks + 52 blendshapes
│   ├── 478→68 landmark conversion
│   ├── Head pose estimation (solvePnP → yaw, pitch, roll)
│   ├── Occlusion scoring (z-depth variance)
│   └── align_and_crop() → [1, 3, 299, 299] normalized tensor
│
├── STAGE 2: Neural Inference
│   ├── Forward pass: XceptionNet([1,3,299,299]) → [fake_prob, real_prob]
│   ├── Temperature scaling: logits / 1.5 → re-softmax
│   └── trust_score = real_prob (calibrated)
│
├── STAGE 3: Tier 1 — Neural Verdict
│   └── trust_score > 0.5 → REAL, else → FAKE
│
├── STAGE 4: Tier 2 — Liveness Verification
│   ├── EAR computation with cosine yaw correction
│   ├── BlinkTracker update (blendshape priority, EAR-DBS fallback)
│   ├── Blink validation (60ms–500ms duration, 70% depth)
│   ├── Pattern analysis (coefficient of variation)
│   └── PASS if valid blink within time window
│
├── STAGE 5: Tier 3 — Forensic Analysis
│   ├── Layer 1: Laplacian variance (sharpness)
│   ├── Layer 2: Distance-texture physics cross-validation
│   ├── Layer 3: Moiré pattern detection (FFT radial profile)
│   ├── Layer 4: Screen light emission (brightness + chrominance + blue)
│   ├── Layer 5: Weak-signal fusion (2+ signals with physical evidence)
│   └── PASS if not suspicious
│
├── STAGE 6: Plugin Analysis
│   ├── Each active plugin: analyze(face, frame) → verdict + confidence
│   ├── Majority voting (>50% FAKE → consensus FAKE)
│   └── Strong consensus (75%+) can override neural
│
├── STAGE 7: Decision Fusion
│   ├── Truth table: T1 × T2 × T3 → base state
│   ├── Plugin integration
│   ├── Hysteresis: 1-frame escalation, 5-frame de-escalation
│   └── REAL → VERIFIED promotion (if confidence > 89% + all tiers PASS)
│
└── STAGE 8: Output
    ├── FaceResult dataclass → EngineResult
    ├── result_queue → main thread
    ├── HUD rendering (bounding boxes, badges, dashboard)
    └── Audit log entry (JSONL)
```

### 9.3 Processing Time Budget (Target)

| Stage | GPU Target | CPU Target |
|---|---|---|
| Face Detection (MediaPipe) | 8–12ms | 15–25ms |
| Neural Inference (XceptionNet) | 15–25ms | 50–80ms |
| Liveness Analysis | 1–2ms | 1–3ms |
| Forensic Analysis | 3–8ms | 5–15ms |
| Plugin Execution | 2–5ms | 3–8ms |
| Decision Fusion | <1ms | <1ms |
| HUD Rendering | 2–5ms | 3–8ms |
| **Total** | **31–58ms** | **77–140ms** |
| **Resulting FPS** | **~17–32 FPS** | **~7–13 FPS** |

---

## 10. Neural Network Specification

### 10.1 Model Card

| Property | Value |
|---|---|
| **Architecture** | XceptionNet (depthwise separable convolutions) |
| **Source** | `timm.create_model('legacy_xception')` |
| **Modification** | Final classifier: `Linear(2048, 2)` + Softmax |
| **Training Dataset** | FaceForensics++ (c23 compression quality) |
| **Manipulation Methods** | Deepfakes, Face2Face, FaceSwap, NeuralTextures |
| **Input Shape** | `[1, 3, 299, 299]` (NCHW, float32) |
| **Output** | `[fake_probability, real_probability]` |
| **PyTorch Size** | 83.5 MB (`ffpp_c23.pth`) |
| **ONNX INT8 Size** | 21.4 MB (`shield_ryzen_int8.onnx`) |
| **Parameters** | ~22.8M |

### 10.2 Input Preprocessing (Critical Path)

The exact preprocessing sequence must be followed — any deviation produces garbage output:

```
Step 1: Face crop (BGR, variable size)
Step 2: BGR → RGB color conversion
Step 3: Resize to 299×299 pixels (bilinear interpolation)
Step 4: Scale to [0, 1]: pixel = pixel / 255.0
Step 5: Normalize to [-1, +1]: pixel = (pixel - 0.5) / 0.5
Step 6: Transpose HWC → CHW: [299, 299, 3] → [3, 299, 299]
Step 7: Add batch dimension: [3, 299, 299] → [1, 3, 299, 299]
```

### 10.3 Confidence Calibration

**Problem:** Raw softmax outputs are overconfident (e.g., 0.99 when true confidence is 0.7).

**Solution:** Temperature scaling with T=1.5

```
# Reverse softmax to get logits
logits = log(raw_softmax_output)

# Apply temperature (T > 1 softens distribution)
scaled_logits = logits / 1.5

# Re-apply softmax
calibrated_output = softmax(scaled_logits)
```

**Effect:**
| Raw Output | After T=1.5 |
|---|---|
| [0.01, 0.99] | [0.04, 0.96] |
| [0.10, 0.90] | [0.16, 0.84] |
| [0.30, 0.70] | [0.35, 0.65] |
| [0.45, 0.55] | [0.47, 0.53] |

### 10.4 Model Security

| Check | Method | Failure Action |
|---|---|---|
| File integrity | SHA-256 hash comparison | `ModelTamperingError` |
| Weight count | Expected tensor count verification | `ModelTamperingError` |
| Shape validation | ONNX input/output shape check | `ModelTamperingError` |
| Runtime verification | Output sanity check (valid probabilities) | Fallback to safe state |

### 10.5 ONNX Export Specification

```python
# Export command (via export_onnx.py)
torch.onnx.export(
    model,
    dummy_input,           # [1, 3, 299, 299] zeros
    "shield_ryzen_int8.onnx",
    opset_version=17,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}
)
```

Target execution provider priority:
1. `VitisAIExecutionProvider` (AMD NPU)
2. `CUDAExecutionProvider` (NVIDIA GPU)
3. `CPUExecutionProvider` (Fallback)

---

## 11. Face Detection & Preprocessing

### 11.1 Primary Detector: MediaPipe FaceLandmarker

| Property | Value |
|---|---|
| **Model** | `face_landmarker_v2_with_blendshapes.task` (3.75 MB) |
| **Landmarks** | 478-point face mesh |
| **Blendshapes** | 52 coefficients (ARKit compatible) |
| **Mode** | VIDEO (temporal smoothing enabled) |
| **Max Faces** | 2 (configurable) |
| **Outputs** | Bounding box, landmarks, blendshapes, transformation matrix |

### 11.2 Fallback Detector: OpenCV DNN SSD

| Property | Value |
|---|---|
| **Model** | OpenCV built-in SSD face detector |
| **Landmarks** | None |
| **Blendshapes** | None |
| **Use Case** | When MediaPipe is unavailable/crashes |

### 11.3 478→68 Landmark Mapping

Standard 68-point face landmark indices mapped from MediaPipe's 478-point mesh:

| Region | Points | Purpose |
|---|---|---|
| Jawline | 17 points | Face boundary detection |
| Right Eyebrow | 5 points | Expression analysis |
| Left Eyebrow | 5 points | Expression analysis |
| Nose Bridge | 4 points | Head pose reference |
| Nose Tip | 5 points | Depth estimation |
| Right Eye | 6 points | EAR computation |
| Left Eye | 6 points | EAR computation |
| Outer Mouth | 12 points | Lip sync plugin |
| Inner Mouth | 8 points | Lip sync plugin |

### 11.4 Head Pose Estimation

**Method:** `cv2.solvePnP()` with 6 reference landmarks (nose tip, chin, left/right eye corners, left/right mouth corners)

**3D Model Points (generic face):**
```python
model_points = np.array([
    (0.0, 0.0, 0.0),        # Nose tip
    (0.0, -330.0, -65.0),   # Chin
    (-225.0, 170.0, -135.0),# Left eye corner
    (225.0, 170.0, -135.0), # Right eye corner
    (-150.0, -150.0, -125.0),# Left mouth corner
    (150.0, -150.0, -125.0) # Right mouth corner
])
```

**Output Ranges:**
| Axis | Range | Reliable |
|---|---|---|
| Yaw (left/right) | -45° to +45° | Yes |
| Pitch (up/down) | -30° to +30° | Yes |
| Roll (tilt) | -30° to +30° | Yes |
| Beyond ranges | — | Degraded accuracy |

### 11.5 Face Alignment & Cropping

```
1. Compute face bounding box from landmarks
2. Expand bbox by 30% (capture forehead, chin)
3. Clamp to frame boundaries
4. Crop face region from original frame
5. Resize to 299×299 (bilinear)
6. Apply normalization pipeline (§10.2)
```

---

## 12. 3-Tier Security Decision Engine

### 12.1 Overview

The decision engine fuses three independent analysis tiers into a single trust verdict. This defense-in-depth approach ensures that compromising any single tier is insufficient to bypass detection.

```
┌─────────────────────────────────────────────────────────┐
│                   Decision Engine                        │
│                                                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │
│  │   TIER 1    │  │   TIER 2    │  │   TIER 3    │    │
│  │   Neural    │  │  Liveness   │  │  Forensic   │    │
│  │             │  │             │  │             │    │
│  │ XceptionNet │  │ Blink (EAR) │  │ 5-Layer     │    │
│  │ confidence  │  │ Blendshapes │  │ Texture     │    │
│  │ calibrated  │  │ Pattern     │  │ Frequency   │    │
│  │             │  │ analysis    │  │ Screen light│    │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘    │
│         │                │                │            │
│         └────────────────┼────────────────┘            │
│                          │                              │
│                   ┌──────▼──────┐                       │
│                   │ Truth Table │                       │
│                   │  + Plugins  │                       │
│                   │ + Hysteresis│                       │
│                   └──────┬──────┘                       │
│                          │                              │
│                   ┌──────▼──────┐                       │
│                   │   Final     │                       │
│                   │   Verdict   │                       │
│                   └─────────────┘                       │
└─────────────────────────────────────────────────────────┘
```

### 12.2 Tier 1: Neural Classification

| Property | Value |
|---|---|
| **Input** | Calibrated trust_score from XceptionNet |
| **Threshold** | 0.5 (post-temperature-scaling) |
| **Output** | REAL if trust_score > 0.5, FAKE otherwise |
| **Latency** | 15–80ms depending on hardware |

### 12.3 Tier 2: Liveness Verification

#### 12.3.1 Eye Aspect Ratio (EAR)

```
        p2    p3
       /        \
p1 ——————————————— p4
       \        /
        p6    p5

EAR = (||p2-p6|| + ||p3-p5||) / (2 × ||p1-p4||)
```

| State | Typical EAR |
|---|---|
| Eyes Open | 0.25–0.35 |
| Mid-blink | 0.15–0.20 |
| Eyes Closed | 0.05–0.15 |

#### 12.3.2 Cosine Angle Compensation

Head rotation foreshortens the horizontal eye dimension:

```
if |yaw| > 20°:
    corrected_EAR = raw_EAR / cos(yaw_radians)
    
if |yaw| > 30°:
    reliability = "LOW"  # EAR values unreliable
```

#### 12.3.3 Dynamic Baseline Scaling (DBS)

Per-user adaptive thresholds that learn the user's natural eye openness:

```python
# Update open-state baseline (fast up, slow down)
if new_ear > current_baseline:
    baseline = 0.9 * baseline + 0.1 * new_ear    # Fast adapt up
else:
    baseline = 0.998 * baseline + 0.002 * new_ear  # Slow decay

# Dynamic thresholds
close_threshold = baseline * 0.65
reopen_threshold = baseline * 0.90
```

#### 12.3.4 Blink Validation Criteria

| Criterion | Threshold | Rationale |
|---|---|---|
| Minimum duration | 60ms | Reject noise/artifacts |
| Maximum duration | 500ms | Reject sustained closure |
| Depth (peak EAR) | < 70% of baseline | Must be a meaningful closure |
| Stuck detection | Auto-reset after 1 second | Prevent deadlocked state |

#### 12.3.5 Blendshape Priority

When MediaPipe blendshapes are available:
- **Index 9:** `eyeBlinkLeft`
- **Index 10:** `eyeBlinkRight`
- **Average > 0.5** → eyes closed (higher quality than EAR)
- Works at all head angles (blendshapes are pose-invariant)

#### 12.3.6 Pattern Analysis

Inter-blink interval analysis detects robotic/pre-recorded blink patterns:

```
intervals = [time between consecutive blinks]
CV = std(intervals) / mean(intervals)

if CV > 0.3: score = 1.0  (natural — varying intervals)
if CV < 0.3: score = CV / 0.3  (robotic — too regular)
if no_blinks > 10 seconds: score = 0.0  (suspicious)
```

#### 12.3.7 Tier 2 Verdict

```
PASS if:
  - Valid blink detected within blink_time_window (10s)
  - Blink pattern score > 0.3 (if enough data)

FAIL if:
  - No blink detected within window
  - OR blink pattern score < 0.3 (robotic)
```

### 12.4 Tier 3: Forensic Analysis

#### 12.4.1 Layer 1 — Laplacian Variance (Sharpness)

```python
forehead_roi = extract_forehead(face, landmarks)
gray = cv2.cvtColor(forehead_roi, cv2.COLOR_BGR2GRAY)
lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()

threshold = device_baseline * 0.4  # or default 15.0

if lap_var < threshold:
    signal = "LOW_TEXTURE"  # Suspicious (blurry/flat)
```

**Detects:** Printed photos, low-resolution deepfakes, heavily compressed video

#### 12.4.2 Layer 2 — Distance-Texture Physics

```python
# Inverse-square law: texture detail decreases with distance²
max_expected = 700 * (50 / distance_cm) ** 2
max_allowed = max_expected * 8.0  # Safety margin

if lap_var > max_allowed:
    signal = "PHYSICS_ANOMALY"  # More detail than possible at this distance
```

**Detects:** Close-up screen replay where texture is too sharp for the apparent distance

#### 12.4.3 Layer 3 — Moiré Pattern Detection

```python
# 2D FFT of forehead grayscale ROI
fft = np.fft.fft2(forehead_gray)
fft_shifted = np.fft.fftshift(fft)
magnitude = np.log1p(np.abs(fft_shifted))

# Build radial frequency profile
radial_profile = azimuthal_average(magnitude)

# Autocorrelation to detect periodic peaks
autocorr = np.correlate(radial_profile, radial_profile, mode='full')

# Screens have regular pixel grids → periodic spikes
if max_peak_height > 0.70:
    signal = "MOIRE_DETECTED"  # Definite screen
```

**Detects:** Screen replay attacks where the screen's pixel grid creates interference patterns

#### 12.4.4 Layer 4 — Screen Light Emission

Three sub-signals combined:

**Brightness Uniformity:**
```python
face_gray = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)
cv_brightness = np.std(face_gray) / np.mean(face_gray)

# Real faces: CV 0.25–0.55 (shadows, contours)
# Screens: CV 0.08–0.18 (uniform backlight)
if cv_brightness < 0.20:
    brightness_signal = high  # Too uniform → screen
```

**Chrominance Range:**
```python
hsv = cv2.cvtColor(face_crop, cv2.COLOR_BGR2HSV)
sat_range = np.percentile(hsv[:,:,1], 95) - np.percentile(hsv[:,:,1], 5)

# Real skin: wide saturation range (shadows, blush, etc.)
# Screens: narrow gamut
if sat_range < threshold:
    chroma_signal = high
```

**Blue Channel Bias:**
```python
mean_b = np.mean(face_crop[:,:,0])
mean_g = np.mean(face_crop[:,:,1])
blue_ratio = mean_b / (mean_g + epsilon)

# LED backlights emit excess blue light
# B/G ratio > 0.92 = suspicious
if blue_ratio > 0.92:
    blue_signal = high
```

**Fusion:** `total = brightness × 0.45 + chroma × 0.35 + blue × 0.20`

#### 12.4.5 Layer 5 — Weak-Signal Fusion

**Critical design principle:** Physics-based signals alone NEVER confirm a screen. Physical evidence (Moiré or screen light) is required.

```python
signals = [laplacian, physics, moire, screen_light]
weak_signals = [s for s in signals if s > 0.35]
physical_evidence = moire > 0.35 or screen_light > 0.35

if len(weak_signals) >= 2 and physical_evidence:
    if average(weak_signals) > 0.45:
        forensic_verdict = FAIL  # Confirmed screen
```

#### 12.4.6 Tier 3 Verdict

```
PASS if:
  - No screen detection confirmed (Layer 5 not triggered)
  - All individual layers below alert thresholds

FAIL if:
  - Layer 5 fusion confirms screen replay
  - OR Layer 3 Moiré score > 0.70 (standalone confirmation)
```

### 12.5 Truth Table Fusion

| Row | T1 (Neural) | T2 (Liveness) | T3 (Forensic) | → Output State |
|-----|-------------|---------------|----------------|----------------|
| 1 | REAL | PASS | PASS | **REAL** (→ VERIFIED if >89% conf) |
| 2 | REAL | PASS | FAIL | **SUSPICIOUS** |
| 3 | REAL | FAIL | PASS | **WAIT_BLINK** |
| 4 | REAL | FAIL | FAIL | **HIGH_RISK** |
| 5 | FAKE | PASS | PASS | **FAKE** |
| 6 | FAKE | PASS | FAIL | **FAKE** |
| 7 | FAKE | FAIL | PASS | **FAKE** |
| 8 | FAKE | FAIL | FAIL | **CRITICAL** |

**Key observations:**
- Neural FAKE always produces FAKE or CRITICAL — never overridden by liveness/forensic alone
- All three must pass for REAL verdict
- VERIFIED requires REAL state + trust_score > 89% confidence threshold

### 12.6 Hysteresis (State Transition Smoothing)

```
ESCALATION (toward threat states):
  → Immediate (1 frame)
  → Transitions to FAKE, CRITICAL, HIGH_RISK happen instantly
  → Rationale: security-first, don't delay threat alerts

DE-ESCALATION (toward safe states):
  → Requires 5 consecutive frames of safe verdict
  → Transitions to REAL, VERIFIED require sustained evidence
  → Rationale: prevent flickering, require confidence before trust
```

---

## 13. Plugin Extensibility Framework

### 13.1 Plugin Interface

```python
class ShieldPlugin(ABC):
    """Abstract base class for all analysis plugins."""
    
    name: str        # Unique identifier (e.g., "heartbeat_rppg")
    tier: str        # Category: "biometric" | "forensic" | "neural" | "temporal"
    
    @abstractmethod
    def analyze(self, face: dict, frame: np.ndarray) -> dict:
        """
        Analyze a face in context of the full frame.
        
        Returns:
            {
                "verdict": "REAL" | "FAKE" | "UNCERTAIN",
                "confidence": float (0.0–1.0),
                "explanation": str
            }
        """
        pass
    
    def release(self) -> None:
        """Optional cleanup on shutdown."""
        pass
```

### 13.2 Plugin Voting Protocol

```
1. Each plugin independently analyzes the face
2. Collect all verdicts (excluding UNCERTAIN)
3. Count REAL vs FAKE votes

Majority Rule (>50% FAKE):
  → plugin_consensus = FAKE
  → Integrated into decision state machine as modifier

Strong Consensus (≥75% FAKE):
  → Can override neural verdict (Tier 1)
  → Triggers immediate escalation

No Consensus:
  → Plugins are advisory only
  → Base truth table verdict used
```

### 13.3 Plugin Catalog

#### 13.3.1 Biometric Plugins

| Plugin | File | Detection Method | Signal |
|---|---|---|---|
| **Heartbeat (rPPG)** | `rppg_heartbeat.py` | Green channel temporal analysis → FFT → peak detection at 42–180 BPM | No pulse = screen/photo |
| **Challenge-Response** | `challenge_response.py` | Randomized "Simon Says" prompts (blink, turn head, smile) → verify compliance | Replay can't respond to random challenges |
| **Stereo Depth** | `stereo_depth.py` | Dual-camera parallax → disparity map → flat surface detection | 2D screens have no depth variation |
| **Skin Reflectance** | `skin_reflectance.py` | Specular highlight detection + gradient texture analysis → material classification | Masks/silicone have different specular response |

#### 13.3.2 Forensic Plugins

| Plugin | File | Detection Method | Signal |
|---|---|---|---|
| **Frequency Analyzer** | `frequency_analyzer.py` | 2D FFT → high-frequency energy ratio → GAN artifact detection | GANs produce characteristic high-frequency patterns |
| **Codec Forensics** | `codec_forensics.py` | 8×8 DCT block boundary detection → double-compression artifacts → BAR score | Re-encoded deepfake streams show blocking |
| **Adversarial Patch** | `adversarial_detector.py` | Sobel gradient → connected component clustering → patch boundary detection | Physical adversarial patches have sharp high-gradient edges |
| **Lip Sync** | `lip_sync_verifier.py` | Mouth landmark tracking → phoneme sequence extraction → temporal consistency check | Pre-recorded video has mismatched lip movement |

#### 13.3.3 Identity Plugin

| Plugin | File | Detection Method | Signal |
|---|---|---|---|
| **ArcFace Re-ID** | `arcface_reid.py` | 512-dimensional face embedding → cosine similarity → identity verification | Unknown/mismatched identity flagged |

### 13.4 Plugin Loading

Plugins are auto-discovered from the `plugins/` directory at startup:
1. Scan for `.py` files
2. Import each module
3. Find classes extending `ShieldPlugin`
4. Instantiate and register
5. Failed plugins are logged but don't block startup

---

## 14. User Interface & HUD

### 14.1 Design Philosophy

**Glassmorphism** — translucent panels with subtle blur, providing a modern cybersecurity aesthetic while keeping the video feed visible beneath UI elements.

### 14.2 HUD Layout

```
┌─────────────────────────────────────────────────────────────┐
│                     VIDEO FEED                               │
│                                                             │
│    ┌─────────────────┐                                      │
│    │  ╔═══════════╗  │                    ┌──────────────┐  │
│    │  ║ VERIFIED  ║  │                    │  DASHBOARD   │  │
│    │  ╚═══════════╝  │                    │              │  │
│    │  ┌───────────┐  │                    │ Neural: 94%  │  │
│    │  │           │  │                    │ ████████░░   │  │
│    │  │   FACE    │  │                    │              │  │
│    │  │   AREA    │  │                    │ EAR: 0.31 A  │  │
│    │  │           │  │                    │ Blinks: 4    │  │
│    │  └───────────┘  │                    │ Pattern: 0.8 │  │
│    │                 │                    │              │  │
│    └─────────────────┘                    │ Dist: 52cm   │  │
│                                           │ Yaw: -5°     │  │
│                                           │ Pitch: 2°    │  │
│                                           │              │  │
│                                           │ T1:✓ T2:✓ T3:✓│  │
│                                           │ Plugins: ●●●○│  │
│                                           └──────────────┘  │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│  ▌ Shield-Ryzen V2  │  Uptime: 00:05:23  │  FPS: 22  │ ● │
└─────────────────────────────────────────────────────────────┘
```

### 14.3 Color System

| State | Hex Color | RGB | Visual Meaning |
|---|---|---|---|
| **VERIFIED** | `#00FF88` | (0, 255, 136) | Confirmed real + live — maximum trust |
| **REAL** | `#90EE90` | (144, 238, 144) | Neural says real, awaiting full verification |
| **WAIT_BLINK** | `#FFD700` | (255, 215, 0) | Needs blink proof — neutral/cautionary |
| **SUSPICIOUS** | `#FF8C00` | (255, 140, 0) | Conflicting signals — elevated alert |
| **HIGH_RISK** | `#FF6600` | (255, 102, 0) | Only neural passes — significant concern |
| **FAKE** | `#FF0000` | (255, 0, 0) | Confirmed deepfake — do not trust |
| **CRITICAL** | `#8B0000` | (139, 0, 0) | All checks failed — maximum threat |

### 14.4 Dashboard Metrics

| Metric | Source | Format |
|---|---|---|
| Neural Confidence | XceptionNet output (calibrated) | Percentage + progress bar |
| EAR Value | Eye Aspect Ratio computation | Float + reliability grade (A/B/C/LOW) |
| Blink Count | BlinkTracker cumulative | Integer |
| Pattern Score | Inter-blink interval CV analysis | Float (0.0–1.0) |
| Distance | Transformation matrix or pinhole model | Centimeters |
| Head Pose | solvePnP output | Yaw/Pitch/Roll in degrees |
| Tier Verdicts | T1/T2/T3 individual results | Checkmarks (✓/✗) |
| Plugin Status | Active plugin health | Colored dots |

### 14.5 Keyboard Controls

| Key | Action |
|---|---|
| `Q` / `ESC` | Exit application (clean shutdown) |
| `F` | Toggle fullscreen / windowed mode |
| `+` | Increase window size |
| `-` | Decrease window size |
| `1` | Preset: 960×540 |
| `2` | Preset: 1280×720 |
| `3` | Preset: 1600×900 |
| `4` | Preset: 1920×1080 |

---

## 15. Security, Privacy & Compliance

### 15.1 Biometric Data Protection

#### 15.1.1 AES-256-GCM Encryption (`shield_crypto.py`)

| Property | Implementation |
|---|---|
| **Algorithm** | AES-256 in Galois/Counter Mode |
| **Key Generation** | Cryptographically random, generated per session |
| **Key Storage** | RAM only — never written to disk |
| **Key Lifecycle** | Created on startup, wiped on exit |
| **What's Encrypted** | Face crops, landmark coordinates, biometric features |
| **When Decrypted** | Only during active processing (microsecond window) |
| **Fallback** | XOR obfuscation if `cryptography` package unavailable |
| **Authentication** | GCM provides built-in tamper detection |

#### 15.1.2 Data Flow Security

```
Camera → Raw Frame (unencrypted, ephemeral)
       → Face Crop → AES-256-GCM Encrypt → Encrypted Buffer
       → Decrypt for Neural Inference → Re-encrypt immediately
       → Results stored as verdicts only (no biometric data)
       → Session end → Key destruction → Data irrecoverable
```

### 15.2 Model Integrity

| Threat | Mitigation |
|---|---|
| Model file replacement | SHA-256 hash verification on every load |
| Weight poisoning | Key count verification (expected tensor count) |
| ONNX structure tampering | Input/output shape validation |
| Runtime manipulation | `ModelTamperingError` exception halts system |

### 15.3 Network Isolation

```
Network Calls Made by Shield-Ryzen V2: ZERO

Verification:
- No import of urllib, requests, httpx, aiohttp
- No socket creation
- No DNS resolution
- MediaPipe model loaded from local .task file
- All inference local (PyTorch / ONNX Runtime)
```

### 15.4 Audit Trail

| Property | Implementation |
|---|---|
| **Format** | JSONL (one JSON object per line) |
| **Contents** | Timestamp, face count, per-face verdicts, FPS, memory |
| **No PII** | Face images NOT logged — only verdicts and scores |
| **Thread Safety** | Mutex-locked file appends |
| **Retention** | Local file, user-controlled |

### 15.5 Privacy Compliance Alignment

| Regulation | Alignment |
|---|---|
| **GDPR** | No personal data transmitted; local processing only; no data retention beyond session |
| **CCPA** | No sale of personal information; no data collection beyond session |
| **Biometric Laws** | No biometric templates stored; ephemeral processing with encryption |

---

## 16. Configuration & Deployment

### 16.1 Configuration File (`config.yaml`)

```yaml
# Security Thresholds
security:
  confidence_threshold: 0.89     # 89% trust threshold for VERIFIED state
  blink_threshold: 0.24          # EAR value indicating eyes closed
  blink_time_window: 10          # Seconds allowed to produce a blink
  laplacian_threshold: 15        # Minimum texture sharpness (Laplacian variance)

# Neural Network Preprocessing
preprocessing:
  input_size: 299                # XceptionNet input resolution (pixels)
  mean: [0.5, 0.5, 0.5]         # FaceForensics++ normalization mean
  std:  [0.5, 0.5, 0.5]         # FaceForensics++ normalization std

# Eye Landmark Indices (MediaPipe 478-mesh)
landmarks:
  left_eye:  [33, 160, 158, 133, 153, 144]
  right_eye: [362, 385, 387, 263, 373, 380]

# MediaPipe Configuration
mediapipe:
  num_faces: 2                                              # Max simultaneous faces
  landmarker_model: "face_landmarker_v2_with_blendshapes.task"  # Model file
```

### 16.2 CLI Arguments

```
usage: start_shield.py [-h] [--cpu] [--source SOURCE] [--windowed]
                       [--size SIZE]

Shield-Ryzen V2 — Real-Time Deepfake Detection

optional arguments:
  -h, --help       Show help message
  --cpu            Force CPU-only inference (no GPU/NPU)
  --source SOURCE  Camera index (default: 0)
  --windowed       Start in windowed mode (default: fullscreen)
  --size SIZE      Window size as WIDTHxHEIGHT (e.g., 1280x720)
```

### 16.3 Deployment Modes

| Mode | Command | Hardware |
|---|---|---|
| **GPU (CUDA)** | `python start_shield.py` | NVIDIA GPU with CUDA |
| **CPU** | `python start_shield.py --cpu` | Any x86_64 CPU |
| **AMD NPU** | `python start_shield.py` (auto-detect) | Ryzen AI with XDNA |
| **Custom Camera** | `python start_shield.py --source 1` | External webcam |
| **Windowed** | `python start_shield.py --windowed --size 1280x720` | Any |

### 16.4 Startup Calibration

On first run (or when `shield_calibration.json` is absent):

```
1. Capture ~100 frames from camera
2. Measure:
   - Laplacian variance baseline (camera's natural sharpness)
   - User's natural EAR range (eye openness baseline)
   - Ambient lighting assessment:
     - GOOD: well-lit, even lighting
     - MODERATE: some shadows, acceptable
     - LOW_LIGHT: may affect accuracy
   - Camera resolution and FPS
3. Save to shield_calibration.json
4. Used for adaptive thresholds throughout session
```

---

## 17. Performance Requirements

### 17.1 Hardware Profiles

| Profile | Hardware | Expected FPS | Memory |
|---|---|---|---|
| **Dev (GPU)** | RTX 3050 Laptop + i7/Ryzen 7 | 15–25 FPS | ~1.2 GB |
| **CPU Only** | Modern x86_64 (8+ cores) | 8–15 FPS | ~800 MB |
| **Target (NPU)** | Ryzen AI (XDNA) | 20–30 FPS (projected) | ~600 MB |

### 17.2 Latency Budget

| Component | Target (GPU) | Target (CPU) |
|---|---|---|
| Camera capture | 5ms | 5ms |
| Face detection | 10ms | 20ms |
| Neural inference | 20ms | 65ms |
| Liveness analysis | 2ms | 3ms |
| Forensic analysis | 5ms | 10ms |
| Plugin execution | 3ms | 5ms |
| Decision fusion | 0.5ms | 0.5ms |
| HUD rendering | 3ms | 5ms |
| **Total end-to-end** | **~48ms (~21 FPS)** | **~113ms (~9 FPS)** |

### 17.3 Resource Limits

| Resource | Limit | Action |
|---|---|---|
| RAM growth | 500 MB above baseline | Trigger garbage collection |
| CPU usage | No hard limit | Best-effort scheduling |
| GPU memory | Model size + 200 MB buffer | Auto-fallback to CPU |
| Disk I/O | Log writes only | <1 MB/hour typical |

---

## 18. Dependencies & Technology Stack

### 18.1 Core Dependencies

| Package | Version | Purpose | Required |
|---|---|---|---|
| `onnxruntime` | 1.18.0 | ONNX model inference | Yes |
| `opencv-python` | 4.9.0.80 | Video capture, image processing | Yes |
| `numpy` | 1.26.4 | Array mathematics | Yes |
| `mediapipe` | 0.10.14 | Face detection + landmarks | Yes |
| `torch` | 2.2.2 | PyTorch model loading, CUDA | Yes |
| `torchvision` | 0.17.2 | Image transforms | Yes |
| `timm` | 1.0.3 | XceptionNet architecture | Yes |
| `psutil` | 5.9.8 | System monitoring | Yes |
| `py-cpuinfo` | 9.0.0 | Hardware detection | Yes |
| `PyYAML` | 6.0.1 | Configuration loading | Yes |
| `Pillow` | 10.3.0 | Image handling | Yes |

### 18.2 Optional Dependencies

| Package | Purpose | Fallback |
|---|---|---|
| `cryptography` | AES-256-GCM encryption | XOR obfuscation |
| `scipy` | Signal processing (rPPG) | Plugin disabled |

### 18.3 Runtime Requirements

| Requirement | Specification |
|---|---|
| **Python** | 3.13 (tested), 3.10+ (compatible) |
| **OS** | Windows 10/11 (primary), Linux (secondary) |
| **Camera** | USB or built-in webcam (640×480 minimum) |
| **RAM** | 4 GB minimum, 8 GB recommended |
| **Storage** | ~200 MB for models + dependencies |

---

## 19. Known Limitations & Constraints

### 19.1 Neural Network Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| Trained on FaceForensics++ only (4 methods) | May miss novel deepfake techniques | Multi-tier defense compensates |
| No diffusion-model training data | Emerging methods may bypass | Plugin framework extensible |
| Single-dataset domain bias | Accuracy varies across conditions | Temperature scaling + calibration |
| No fine-tuning on real webcam data | Distribution shift possible | Startup calibration adapts thresholds |

### 19.2 Face Detection Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| MediaPipe fails at >45° yaw | Face lost at extreme angles | Fallback to DNN SSD |
| Small faces at >1.5m distance | May not detect distant faces | Distance-aware processing |
| Maximum 2 simultaneous faces | Not suitable for crowds | Configurable (performance trade-off) |
| No face recognition | Cannot identify specific people | ArcFace plugin (optional) |

### 19.3 Liveness Detection Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| EAR unreliable at >30° yaw | Marked as LOW reliability | Blendshape fallback |
| 4-second baseline establishment | Initial period has no blink data | WAIT_BLINK state |
| Infrequent blinkers | May stay in WAIT_BLINK | Extended time window (10s) |
| Glasses affect specular analysis | Skin reflectance plugin less reliable | Multiple plugin voting |

### 19.4 Forensic Analysis Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| LCD/OLED calibrated only | May miss projector replay | Moiré detection generalized |
| Distance-dependent Moiré | Varies with camera-screen angle | Multi-signal fusion |
| LED lighting → blue false positives | Indoor lighting triggers blue bias | Layer 5 requires physical evidence |
| Approximate distance model | Inverse-square assumption | 8× safety margin |

### 19.5 Performance Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| CPU-only ~8-15 FPS | May feel sluggish | Frame dropping (latest-frame processing) |
| Memory growth over sessions | Long sessions need GC | 500 MB threshold trigger |
| ONNX INT8 accuracy loss | Edge cases may regress | Calibrated quantization |

### 19.6 Security Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| Python GC doesn't guarantee memory clearing | Encrypted data may persist in freed memory | Ephemeral keys + process exit |
| XOR fallback if no `cryptography` | Weak obfuscation only | Document as known limitation |
| Model reverse-engineering possible | Architecture known (open source) | Hash verification prevents tampering |
| Targeted adversarial attacks | XceptionNet-specific attacks may bypass | Multi-tier + plugin defense |

### 19.7 Hardware Limitations

| Limitation | Impact | Mitigation |
|---|---|---|
| NVIDIA CUDA dev environment | Not tested on AMD GPU | ONNX Runtime abstracts backend |
| AMD NPU untested in production | Theoretical performance only | ONNX export validated |
| Single camera setup | No true depth verification | Stereo depth plugin (skeleton) |

---

## 20. Future Roadmap

### 20.1 Short-Term (Next 3 Months)

| Priority | Enhancement | Description |
|---|---|---|
| P0 | AMD NPU deployment | Full Vitis AI quantization, benchmark on Ryzen AI hardware |
| P0 | Multi-dataset training | Add CelebDF, DFDC, WildDeepfake data |
| P1 | Temporal analysis | Analyze face consistency across 2–5 second windows |
| P1 | Grad-CAM visualization | Show which face regions trigger detection |
| P2 | Audio-visual sync | Correlate lip movement with speech audio |

### 20.2 Medium-Term (3–6 Months)

| Priority | Enhancement | Description |
|---|---|---|
| P1 | Multi-camera fusion | Production dual-camera stereo depth verification |
| P1 | Continual learning | Online adaptation to new deepfake methods |
| P2 | Multi-face scaling | Handle 5+ faces for meeting/conference scenarios |
| P2 | API server mode | REST API for integration into other applications |

### 20.3 Long-Term (6–12 Months)

| Priority | Enhancement | Description |
|---|---|---|
| P1 | Mobile deployment | Android/iOS via ONNX Runtime Mobile |
| P2 | Browser extension | WebRTC integration for video call protection |
| P2 | Diffusion model detection | Specialized training for Stable Diffusion face swaps |
| P3 | Federated learning | Privacy-preserving model updates across devices |

### 20.4 Alternative Model Exploration

| Model | Potential Benefit | Trade-off |
|---|---|---|
| **EfficientNet-B4** | Smaller, faster inference | Less proven on FF++ |
| **Vision Transformer (ViT)** | Better cross-domain generalization | Heavier, ONNX-unfriendly |
| **Capsule Networks** | Rotation invariant features | Slow inference |
| **Multi-task CNN** | Joint detection + classification | Complex training pipeline |

### 20.5 Alternative Face Detector Exploration

| Detector | Potential Benefit | Trade-off |
|---|---|---|
| **RetinaFace** | More accurate landmarks | Heavier model |
| **SCRFD** | Very fast inference | Fewer landmarks available |
| **YOLOv8-Face** | Real-time performance | No mesh landmarks |
| **Dlib HOG+68** | CPU-friendly | Poor performance at angles |

---

## 21. Success Metrics

### 21.1 Detection Accuracy

| Metric | Target | Measurement |
|---|---|---|
| True Positive Rate (deepfake detected) | ≥85% | FaceForensics++ c23 test set |
| True Negative Rate (real verified) | ≥90% | Live webcam testing |
| False Positive Rate (real flagged as fake) | ≤10% | Live webcam testing, diverse subjects |
| False Negative Rate (fake missed) | ≤15% | FF++ + screen replay + photo attacks |
| Liveness detection rate | ≥95% | Live subjects blink within 10 seconds |
| Screen replay detection | ≥80% | Phone/tablet/monitor replay testing |

### 21.2 Performance

| Metric | Target | Measurement |
|---|---|---|
| GPU FPS (RTX 3050) | ≥15 FPS | 5-minute sustained run |
| CPU FPS | ≥8 FPS | 5-minute sustained run |
| End-to-end latency | ≤200ms | Camera to HUD display |
| Startup time | ≤15s | Cold start including calibration |
| Memory stability | <500 MB growth/hour | 1-hour stress test |

### 21.3 User Experience

| Metric | Target | Measurement |
|---|---|---|
| Time to first verdict | ≤5 seconds | After face detection |
| Time to VERIFIED state | ≤15 seconds | From cold start |
| False state flickering | 0 occurrences | 5-minute stable session |
| Verdict clarity | 100% interpretable | Color + text badges always visible |

### 21.4 Security

| Metric | Target | Measurement |
|---|---|---|
| Network connections | 0 | Verified via traffic monitoring |
| Biometric data on disk | 0 bytes | File system audit after session |
| Model integrity checks | 100% pass rate | SHA-256 verification on every load |
| Encryption coverage | 100% of face data | Code audit of data flow |

---

## 22. Glossary

| Term | Definition |
|---|---|
| **EAR** | Eye Aspect Ratio — ratio of vertical eye opening to horizontal width; used for blink detection |
| **rPPG** | Remote Photoplethysmography — non-contact technique to detect blood flow pulse from subtle skin color changes captured by camera |
| **FFT** | Fast Fourier Transform — algorithm to decompose an image or signal into its frequency components |
| **Moiré** | Interference pattern produced when a camera captures a screen's pixel grid, creating visible periodic artifacts |
| **Laplacian** | Second-order derivative operator that measures the rate of intensity change in an image; high values indicate sharp edges |
| **DBS** | Dynamic Baseline Scaling — adaptive algorithm that learns a user's personal eye openness baseline and adjusts thresholds accordingly |
| **HFER** | High-Frequency Energy Ratio — ratio of high-frequency to total energy in a frequency spectrum; elevated in GAN-generated images |
| **BAR** | Blocking Artifact Ratio — measure of 8×8 block boundary artifacts indicating JPEG/H.264 double compression |
| **SNR** | Signal-to-Noise Ratio — measure of signal strength relative to background noise; higher is better |
| **GCM** | Galois/Counter Mode — authenticated encryption mode providing both confidentiality and integrity verification |
| **NCHW** | Tensor dimension ordering: Batch × Channels × Height × Width; standard format for PyTorch and ONNX |
| **ONNX** | Open Neural Network Exchange — portable, vendor-neutral format for representing machine learning models |
| **XDNA** | AMD's AI engine architecture powering the Neural Processing Unit (NPU) in Ryzen AI processors |
| **Vitis AI** | AMD's development platform for AI inference optimization on AMD hardware (FPGA, NPU) |
| **Hysteresis** | Technique of requiring sustained evidence before changing state, preventing rapid oscillation between states |
| **Temperature Scaling** | Post-hoc calibration technique that divides logits by a temperature parameter to reduce overconfidence in neural network outputs |
| **Deepfake** | AI-generated or AI-manipulated media (video, image, audio) designed to impersonate a real person |
| **Presentation Attack** | Attempt to deceive a biometric system using a fabricated artifact (photo, screen, mask, replay) |
| **Blendshape** | Parametric facial expression coefficient (0–1) representing the activation of specific facial muscles (e.g., eye blink, smile) |
| **IoU** | Intersection over Union — metric measuring overlap between two bounding boxes; used for face tracking across frames |

---
# Shield-Ryzen V2 — Complete Prototype Documentation
## 100% Knowledge Reference | AMD Slingshot 2026
### Developer: Inayat Hussain

---

# TABLE OF CONTENTS

1. [Project Identity](#1-project-identity)
2. [What This Prototype Does](#2-what-this-prototype-does)
3. [Architecture Overview](#3-architecture-overview)
4. [File-by-File Breakdown](#4-file-by-file-breakdown)
5. [The Complete Processing Pipeline](#5-the-complete-processing-pipeline)
6. [Neural Network: XceptionNet](#6-neural-network-xceptionnet)
7. [Face Detection & Preprocessing](#7-face-detection--preprocessing)
8. [The 3-Tier Security Decision System](#8-the-3-tier-security-decision-system)
9. [Plugin System](#9-plugin-system)
10. [Utility Algorithms & Math](#10-utility-algorithms--math)
11. [HUD Display System](#11-hud-display-system)
12. [Security & Privacy Features](#12-security--privacy-features)
13. [Configuration System](#13-configuration-system)
14. [How to Run](#14-how-to-run)
15. [Limitations](#15-limitations)
16. [Alternatives & Future Work](#16-alternatives--future-work)
17. [Dependencies](#17-dependencies)
18. [Glossary](#18-glossary)
19. [Complete Deep-Dive Folder Structure](#19-complete-deep-dive-folder-structure)

---

# 1. PROJECT IDENTITY

| Field | Value |
|---|---|
| **Name** | Shield-Ryzen V2 (Shield-Xception) |
| **Competition** | AMD Slingshot 2026 |
| **Developer** | Inayat Hussain |
| **Purpose** | Real-time deepfake detection via webcam |
| **Core Model** | XceptionNet (FaceForensics++ c23 weights) |
| **Dev GPU** | NVIDIA RTX 3050 Laptop GPU (CUDA) |
| **Target Hardware** | AMD Ryzen AI NPU (via ONNX Runtime) |
| **Inference** | 100% LOCAL — no cloud, no internet |
| **Language** | Python 3.13 |
| **Framework** | PyTorch + timm + ONNX Runtime |

---

# 2. WHAT THIS PROTOTYPE DOES

Shield-Ryzen V2 is a **real-time deepfake detection system** that runs entirely on your local machine. It captures video from your webcam, detects faces, and determines whether each face is **REAL** (a live human) or **FAKE** (a deepfake, screen replay, or mask).

**In plain English:** You sit in front of your webcam. The system draws a box around your face and shows a trust verdict:
- **Green "VERIFIED"** = You are a real, live human
- **Red "FAKE"** = The system detected a deepfake or presentation attack
- **Orange "SUSPICIOUS"** = Something is off but not confirmed fake
- **Yellow "WAIT_BLINK"** = Waiting for you to blink to prove liveness

It does this by combining:
1. A **neural network** (XceptionNet) that classifies face images as real/fake
2. **Liveness checks** (blink detection, eye tracking)
3. **Forensic analysis** (texture sharpness, frequency spectrum, screen light detection)
4. **Plugin-based analysis** (heartbeat detection, adversarial patch detection, etc.)

---

# 3. ARCHITECTURE OVERVIEW

## 3.1 Triple-Buffer Async Pipeline

The system runs on **3 parallel threads** for maximum performance:

```
┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│ CAMERA THREAD│────>│   AI THREAD  │────>│  MAIN THREAD  │
│  (Producer)  │     │ (Processor)  │     │  (HUD/Display)│
│              │     │              │     │               │
│ Captures     │     │ Face detect  │     │ Renders HUD   │
│ frames from  │     │ Neural infer │     │ Shows window  │
│ webcam       │     │ Plugin votes │     │ Handles keys  │
│              │     │ State machine│     │               │
│ Validates    │     │ Audit log    │     │ cv2.imshow()  │
│ frame quality│     │              │     │               │
└──────────────┘     └──────────────┘     └──────────────┘
     │                     │                     │
     └── camera_queue ─────┘── result_queue ─────┘
         (maxsize=1)           (maxsize=1)
```

**Why maxsize=1?** Single-frame buffers ensure the system always processes the **latest** frame, not stale ones. This minimizes latency at the cost of dropping frames when the AI thread is busy.

## 3.2 Data Flow (One Frame)

```
Webcam → ShieldCamera.read_validated_frame()
       → camera_queue
       → ShieldEngine._ai_thread_loop()
         ├── ShieldFacePipeline.detect_faces(frame)
         │   ├── MediaPipe FaceLandmarker → 478 landmarks
         │   ├── Convert to 68-point landmarks
         │   ├── Head pose estimation (yaw/pitch/roll)
         │   ├── Occlusion scoring
         │   └── align_and_crop() → 299×299 normalized tensor
         ├── Neural inference (ONNX or PyTorch)
         │   └── XceptionNet → [fake_prob, real_prob]
         ├── ConfidenceCalibrator.calibrate() (temperature scaling)
         ├── Tier 1: Neural verdict (REAL/FAKE)
         ├── Tier 2: Liveness check
         │   ├── compute_ear() with cosine correction
         │   ├── BlinkTracker.update() (blendshape or EAR-DBS)
         │   └── Blink pattern analysis
         ├── Tier 3: Forensic check
         │   ├── compute_texture_score() (5-layer screen detection)
         │   └── Frequency analysis, Moiré, screen light
         ├── Plugin Analysis (heartbeat, skin, codec, etc.)
         ├── DecisionStateMachine.update(T1, T2, T3, plugins)
         │   └── Truth table + agile hysteresis
         └── FaceResult → result_queue
       → ShieldHUD.render(frame, engine_result)
       → cv2.imshow() → Screen
```

---

# 4. FILE-BY-FILE BREAKDOWN

## Core Engine Files

| File | Lines | Role |
|---|---|---|
| `shield_engine.py` | 897 | Central orchestrator — threads, pipeline, state machine |
| `shield_xception.py` | 512 | XceptionNet model class + SHA-256 integrity verification |
| `shield_face_pipeline.py` | 900 | Face detection (MediaPipe/DNN), landmarks, normalization |
| `shield_utils_core.py` | 1269 | EAR, texture analysis, calibration, state machine, blink tracker |
| `shield_camera.py` | 326 | Camera capture with validation and health monitoring |
| `shield_hud.py` | 536 | Glassmorphism HUD overlay rendering |
| `shield_crypto.py` | 114 | AES-256-GCM biometric encryption |
| `shield_logger.py` | 108 | JSONL structured audit logging |
| `shield_plugin.py` | 69 | Abstract base class for plugins |
| `shield_types.py` | 32 | FaceResult and EngineResult dataclasses |
| `start_shield.py` | 260 | Main launcher with CLI arguments |
| `v3_xdna_engine.py` | 84 | AMD XDNA NPU engine subclass |
| `export_onnx.py` | 76 | PyTorch → ONNX conversion script |
| `config.yaml` | 45 | Runtime configuration |

## Plugin Files (in `plugins/`)

| File | Lines | Purpose |
|---|---|---|
| `rppg_heartbeat.py` | 172 | Remote photoplethysmography (heartbeat from skin color) |
| `challenge_response.py` | 163 | "Simon Says" liveness (blink, turn head, smile) |
| `stereo_depth.py` | 133 | Dual-camera 3D depth verification |
| `skin_reflectance.py` | 119 | Specular highlight + gradient texture analysis |
| `codec_forensics.py` | 118 | JPEG/H.264 double-compression detection |
| `frequency_analyzer.py` | 117 | FFT-based GAN artifact detection |
| `adversarial_detector.py` | 113 | Physical adversarial patch detection |
| `lip_sync_verifier.py` | 105 | Lip-reading phoneme verification |
| `arcface_reid.py` | 149 | ArcFace identity re-identification |

---

# 5. THE COMPLETE PROCESSING PIPELINE

## 5.1 Startup Sequence

1. `start_shield.py` parses CLI args (`--cpu`, `--source`, `--windowed`, etc.)
2. Creates `ShieldEngine(config)` which:
   - Initializes `ShieldCamera` (DirectShow backend on Windows)
   - Initializes `ShieldFacePipeline` (loads MediaPipe `face_landmarker_v2_with_blendshapes.task`)
   - Loads neural model: prefers `shield_ryzen_int8.onnx` (ONNX), falls back to `ffpp_c23.pth` (PyTorch)
   - SHA-256 hash verifies model integrity
   - Creates `ConfidenceCalibrator(temperature=1.5)`
   - Creates `DecisionStateMachine(frames=5)` per face
   - Creates `BlinkTracker` per face
   - Loads all plugins from `plugins/` directory
   - Runs startup calibration (measures camera baseline)
3. `engine.start()` launches camera thread + AI thread
4. Main loop: `engine.get_latest_result()` → `hud.render()` → `cv2.imshow()`

## 5.2 Per-Frame Processing (AI Thread)

For each frame from the camera queue:

### Stage 1: Face Detection
- MediaPipe FaceLandmarker processes the RGB frame
- Returns up to 2 faces (configurable) with 478-point mesh landmarks
- Converts 478→68 landmark mapping for EAR compatibility
- Estimates head pose: yaw (left/right), pitch (up/down), roll (tilt)
- Computes occlusion score from landmark depth variance
- `align_and_crop()`: expands bbox, crops face, resizes to 299×299, BGR→RGB, normalizes to [-1,1], transposes to NCHW

### Stage 2: Neural Inference
- Input: `[1, 3, 299, 299]` float32 tensor (normalized [-1, 1])
- Model outputs `[fake_prob, real_prob]` via softmax
- Temperature scaling: `logits / 1.5` then re-softmax (prevents overconfidence)
- Trust score = `real_prob` (inverted: 1.0 = trusted real, 0.0 = fake)

### Stage 3: Three-Tier Decision

**Tier 1 — Neural:** `trust_score > 0.5` → REAL, else FAKE

**Tier 2 — Liveness:**
- Compute EAR (Eye Aspect Ratio) with cosine angle compensation
- Track blinks via blendshapes (priority) or EAR-DBS (fallback)
- PASS if blink detected within time window

**Tier 3 — Forensic:**
- 5-layer texture analysis:
  - Layer 1: Laplacian variance (sharpness)
  - Layer 2: Distance-texture physics cross-validation
  - Layer 3: Moiré pattern detection (FFT radial profile)
  - Layer 4: Screen light emission (brightness uniformity, chrominance, blue bias)
  - Layer 5: Weak-signal fusion (2+ marginal signals = confirm)
- PASS if not suspicious

### Stage 4: Truth Table Fusion

```
Neural | Liveness | Forensic | → State
-------|----------|----------|--------
REAL   | PASS     | PASS     | REAL → promoted to VERIFIED by engine
REAL   | PASS     | FAIL     | SUSPICIOUS
REAL   | FAIL     | PASS     | WAIT_BLINK
REAL   | FAIL     | FAIL     | HIGH_RISK
FAKE   | PASS     | PASS     | FAKE
FAKE   | PASS     | FAIL     | FAKE
FAKE   | FAIL     | PASS     | FAKE
FAKE   | FAIL     | FAIL     | CRITICAL
```

### Stage 5: Hysteresis
- **Escalation** (→ FAKE/CRITICAL/HIGH_RISK): Immediate (1 frame)
- **De-escalation** (→ REAL/VERIFIED): Requires 5 consecutive frames
- Prevents flickering between states

---

# 6. NEURAL NETWORK: XCEPTIONNET

## 6.1 Architecture
- **Base:** XceptionNet from `timm.create_model('legacy_xception')`
- **Modification:** Final classifier replaced with `Linear(2048, 2)` + Softmax
- **Output:** `[fake_probability, real_probability]`
- **Training Data:** FaceForensics++ dataset, quality c23 (compressed)
- **Weights File:** `ffpp_c23.pth` (83.5 MB, PyTorch) or `shield_ryzen_int8.onnx` (21.4 MB, quantized)

## 6.2 Why XceptionNet?
- Depthwise separable convolutions are efficient for mobile/edge
- Proven state-of-the-art on FaceForensics++ benchmark
- ONNX-exportable for AMD NPU deployment
- Good balance of accuracy vs. inference speed

## 6.3 Normalization (CRITICAL)
```
Input: BGR face crop from webcam
1. BGR → RGB conversion
2. Resize to 299×299
3. Scale to [0, 1]: pixel / 255.0
4. Normalize: (pixel - 0.5) / 0.5
5. Result: [-1.0, +1.0] range (FaceForensics++ standard)
6. Transpose: HWC → CHW → NCHW [1, 3, 299, 299]
```
**If normalization is wrong, the model produces garbage.** This is the #1 bug source.

## 6.4 Model Security
- SHA-256 hash verification on load (prevents tampering)
- Key count verification (expected number of weight tensors)
- Shape verification for ONNX models
- `ModelTamperingError` raised if any check fails

## 6.5 Temperature Scaling
Raw softmax outputs are overconfident (e.g., 0.99 when should be 0.7).
```
logits = log(softmax_probs)
scaled_logits = logits / 1.5   # temperature > 1 = softer
calibrated = softmax(scaled_logits)
```
This makes the 89% confidence threshold actually meaningful.

---

# 7. FACE DETECTION & PREPROCESSING

## 7.1 MediaPipe FaceLandmarker (Primary)
- Model: `face_landmarker_v2_with_blendshapes.task` (3.75 MB)
- 478-point face mesh with 52 blendshape coefficients
- Provides: bounding box, landmarks, head pose, blendshapes, transformation matrix
- Runs in VIDEO mode for temporal smoothing

## 7.2 DNN SSD (Fallback)
- OpenCV's DNN module with SSD face detector
- No landmarks, no blendshapes, no head pose
- Used only when MediaPipe is unavailable

## 7.3 478→68 Landmark Mapping
MediaPipe provides 478 mesh points. Standard face analysis uses 68.
The mapping selects specific mesh indices to approximate the 68-point Dlib standard:
- Jawline: 17 points (indices 10, 338, 297, ...)
- Eyebrows: 10 points
- Nose: 9 points
- Eyes: 12 points (6 per eye)
- Mouth: 20 points

## 7.4 Head Pose Estimation
Computed from nose tip, chin, eye corners, and mouth corners landmarks using `cv2.solvePnP()`:
- **Yaw:** Left/right rotation (-45° to +45°)
- **Pitch:** Up/down rotation (-30° to +30°)
- **Roll:** Head tilt

## 7.5 Occlusion Scoring
Measures how much of the face is occluded (hand, hair, mask edge):
- Examines z-depth variance of forehead, chin, cheek landmarks
- High variance = likely occluded
- Used to reduce confidence in EAR and neural results

---

# 8. THE 3-TIER SECURITY DECISION SYSTEM

## Tier 1: Neural Verdict
- XceptionNet classifies face as real/fake
- Temperature-calibrated confidence
- Threshold: trust_score > 0.5 → REAL

## Tier 2: Liveness (Blink Detection)

### EAR Formula
```
EAR = (|p2-p6| + |p3-p5|) / (2 × |p1-p4|)
```
Where p1-p6 are the 6 eye landmark points (outer, upper1, upper2, inner, lower2, lower1).

### Cosine Angle Compensation
At non-frontal angles, the horizontal eye distance foreshortens:
```
if |yaw| > 20°: corrected_EAR = raw_EAR / cos(yaw_radians)
if |yaw| > 30°: reliability = "LOW" (EAR unreliable)
```

### Dynamic Baseline Scaling (DBS)
Instead of a fixed threshold, the system learns YOUR eye openness:
```
open_state_ear = 0.9 × old + 0.1 × new  (if new > old, fast adapt up)
open_state_ear = 0.998 × old + 0.002 × new  (slow decay down)
close_threshold = open_state_ear × 0.65
reopen_threshold = open_state_ear × 0.90
```

### Blink Validation
- Duration: 60ms–500ms (reject noise < 60ms, reject long close > 500ms)
- Depth: peak EAR must be < 70% of open baseline
- Stuck detection: auto-reset if "in blink" > 1 second

### Blendshape Priority
If MediaPipe blendshapes are available (indices 9=EyeBlinkLeft, 10=EyeBlinkRight):
- Average > 0.5 = eyes closed
- Higher quality than EAR (works at all angles)

### Pattern Analysis
- Coefficient of variation (CV) of inter-blink intervals
- CV > 0.3 = natural (varying intervals), score 1.0
- CV < 0.3 = robotic (too regular), score proportional
- No blinks for 10+ seconds = suspicious

## Tier 3: Forensic (Texture + Frequency)

### Layer 1: Laplacian Variance
```
lap_var = cv2.Laplacian(forehead_gray, CV_64F).var()
threshold = device_baseline × 0.4  (or default 15.0)
```
Low variance = blurry/smooth = suspicious (printed photo, low-res deepfake)

### Layer 2: Distance-Texture Physics
```
max_expected = 700 × (50 / distance_cm)²
max_allowed = max_expected × 8.0  (safety margin)
```
If texture exceeds physics-based maximum at given distance → signal (not confirmation alone)

### Layer 3: Moiré Detection
- 2D FFT of forehead ROI
- Build radial frequency profile
- Autocorrelation to detect periodic peaks
- Screens have regular pixel grids → periodic frequency spikes
- Score > 0.70 = definite screen

### Layer 4: Screen Light Detection
Three sub-signals:
- **Brightness uniformity:** Real faces have shadows (CV 0.25-0.55), screens are uniform (CV 0.08-0.18)
- **Chrominance range:** Real skin has rich color variation, screens have narrow gamut
- **Blue channel bias:** LED backlights emit excess blue (B/G ratio > 0.92)

Combined: `total = brightness×0.45 + chroma×0.35 + blue×0.20`

### Layer 5: Weak-Signal Fusion
- Requires **physical evidence** (Moiré or screen light) — physics alone is NEVER enough
- 2+ signals above 0.35 with at least one physical evidence signal
- Combined average > 0.45 → confirm screen replay

---

# 9. PLUGIN SYSTEM

## 9.1 Architecture
All plugins extend `ShieldPlugin` (abstract base class):
```python
class ShieldPlugin(ABC):
    name: str        # e.g., "heartbeat_rppg"
    tier: str        # "biometric", "forensic", "neural", "temporal"
    analyze(face, frame) → dict  # Returns verdict, confidence, explanation
    release()        # Cleanup
```

## 9.2 Plugin Voting
- Each plugin returns: `{"verdict": "REAL"|"FAKE"|"UNCERTAIN", "confidence": float}`
- **Majority rule:** If >50% of plugins vote FAKE → plugin consensus = FAKE
- **Strong consensus (75%+):** Can override neural verdict
- UNCERTAIN votes are excluded from counts

## 9.3 Active Plugins

| Plugin | Type | What It Detects |
|---|---|---|
| **Heartbeat (rPPG)** | Biometric | Blood flow pulse via green channel FFT (42-180 BPM) |
| **Challenge-Response** | Biometric | Replay attacks via "Simon Says" actions |
| **Stereo Depth** | Biometric | 2D flat screens via dual-camera parallax |
| **Skin Reflectance** | Biometric | Masks via specular/gradient analysis |
| **Frequency Analyzer** | Forensic | GAN artifacts via 2D FFT high-frequency ratio |
| **Codec Forensics** | Forensic | Re-encoded streams via 8×8 blocking artifacts |
| **Adversarial Patch** | Forensic | Physical patches via Sobel gradient clustering |
| **Lip Sync** | Forensic | Pre-recorded video via phoneme verification |
| **ArcFace Re-ID** | Identity | Unknown users via 512-d embedding matching |

---

# 10. UTILITY ALGORITHMS & MATH

## 10.1 Distance Estimation
Two methods:
1. **Transformation matrix** (preferred): `z_cm = matrix[2, 3]` from MediaPipe
2. **Pinhole camera model** (fallback): `D = (F × 14cm) / bbox_width_px`
   - Assumes average face width = 14cm
   - Focal length ≈ frame width (approximation)

## 10.2 Device Calibration
At startup, captures ~100 frames to measure:
- Laplacian variance baseline (camera sharpness)
- User's natural EAR range
- Lighting condition (GOOD/MODERATE/LOW_LIGHT)
- Camera resolution
Saves to `shield_calibration.json` for runtime use.

## 10.3 Signal Smoother
Exponential moving average per-face:
```
smoothed = α × new + (1-α) × old
α = 0.3 for neural confidence
α = 0.15 for texture scores
```

## 10.4 Face Tracking
IoU (Intersection over Union) based tracker assigns persistent IDs to faces across frames. Stale trackers are purged when faces disappear.

---

# 11. HUD DISPLAY SYSTEM

## 11.1 Design
- **Glassmorphism** style with translucent panels
- Color-coded state badges above each face
- Right-side dashboard with detailed metrics
- Bottom bar with uptime and camera health

## 11.2 Color Palette
| State | Color | Meaning |
|---|---|---|
| VERIFIED | Green (#00FF88) | Confirmed real + live |
| REAL | Light Green | Neural says real, awaiting full verification |
| WAIT_BLINK | Yellow (#FFD700) | Needs blink for liveness |
| SUSPICIOUS | Orange (#FF8C00) | Conflicting signals |
| HIGH_RISK | Dark Orange | Only neural passes |
| FAKE | Red (#FF0000) | Confirmed deepfake |
| CRITICAL | Dark Red (#8B0000) | All checks failed |

## 11.3 Dashboard Metrics
- Neural confidence (progress bar)
- EAR value + reliability grade
- Blink count + pattern score
- Distance (cm)
- Head pose (yaw/pitch/roll)
- Tier verdicts (T1/T2/T3)
- Plugin status (dots)

---

# 12. SECURITY & PRIVACY FEATURES

## 12.1 Biometric Encryption (`shield_crypto.py`)
- **AES-256-GCM** authenticated encryption
- Ephemeral keys (generated per session, never stored to disk)
- All face crops and landmarks encrypted in RAM
- Decrypted only during active processing
- Keys securely wiped on exit

## 12.2 Model Integrity
- SHA-256 hash verification on model load
- `ModelTamperingError` if hash mismatch
- Key count and shape verification

## 12.3 Audit Logging (`shield_logger.py`)
- JSONL format (one JSON object per line)
- Every frame's decision logged with:
  - Timestamp, face count, per-face results
  - FPS, timing breakdown, memory usage
- Thread-safe (mutex-locked file writes)

## 12.4 Privacy Promise
- **100% local processing** — no network calls, no cloud
- No face images stored to disk (encrypted in-memory only)
- No telemetry, no analytics
- Designed for GDPR/CCPA compliance

---

# 13. CONFIGURATION SYSTEM

## `config.yaml` Key Settings
```yaml
security:
  confidence_threshold: 0.89   # 89% rule for verification
  blink_threshold: 0.24        # EAR closed threshold
  blink_time_window: 10        # Seconds to detect a blink
  laplacian_threshold: 15      # Minimum texture sharpness

preprocessing:
  input_size: 299              # XceptionNet input resolution
  mean: [0.5, 0.5, 0.5]       # FF++ normalization
  std:  [0.5, 0.5, 0.5]

landmarks:
  left_eye:  [33, 160, 158, 133, 153, 144]
  right_eye: [362, 385, 387, 263, 373, 380]

mediapipe:
  num_faces: 2
  landmarker_model: "face_landmarker_v2_with_blendshapes.task"
```

---

# 14. HOW TO RUN

```bash
# Install dependencies
pip install -r requirements.txt

# Run with CPU (standard engine)
python start_shield.py --cpu

# Run with specific camera
python start_shield.py --cpu --source 1

# Run windowed
python start_shield.py --cpu --windowed --size 1280x720

# Run with AMD NPU optimization
python start_shield.py
```

**Keyboard Controls:**
- `Q` or `ESC`: Exit
- `F`: Toggle fullscreen
- `+`/`-`: Resize window
- `1`-`4`: Preset sizes (960×540 to 1920×1080)

---

# 15. LIMITATIONS

## 15.1 Neural Network
- Trained only on FaceForensics++ (4 manipulation methods)
- May not generalize to novel deepfake techniques (diffusion models, etc.)
- Single-dataset training = domain bias
- No fine-tuning on real-world webcam data

## 15.2 Face Detection
- MediaPipe struggles with extreme angles (>45° yaw)
- Small faces at distance (>1.5m) may not be detected
- Only processes up to 2 faces simultaneously
- No face recognition (just detection + analysis)

## 15.3 Liveness Detection
- EAR unreliable at >30° yaw (marked LOW reliability)
- Blink detection requires ~4 seconds of observation to establish baseline
- People who rarely blink may trigger WAIT_BLINK
- Glasses can affect specular reflection analysis

## 15.4 Forensic Analysis
- Screen replay detection calibrated for LCD/OLED — may miss projectors
- Moiré detection depends on camera-screen distance and angle
- Indoor LED lighting can trigger false positives on blue channel analysis
- Physics-based distance model is approximate (inverse-square assumption)

## 15.5 Performance
- Real-time at ~15-25 FPS on RTX 3050 (GPU) or ~8-15 FPS (CPU)
- Memory grows over long sessions (GC triggered at 500MB growth)
- ONNX INT8 model is ~4x smaller but may lose accuracy on edge cases

## 15.6 Security
- Python's garbage collector doesn't guarantee memory clearing
- XOR fallback if `cryptography` package missing (weak obfuscation)
- Model weights could theoretically be reverse-engineered
- Adversarial attacks specifically targeting XceptionNet may bypass detection

## 15.7 Hardware
- Currently optimized for NVIDIA CUDA (dev environment)
- AMD NPU deployment via Vitis AI EP is untested in production
- Single-camera setup limits depth-based anti-spoofing

---

# 16. ALTERNATIVES & FUTURE WORK

## 16.1 Alternative Models
| Alternative | Pros | Cons |
|---|---|---|
| EfficientNet-B4 | Smaller, faster | Less proven on FF++ |
| Vision Transformer (ViT) | Better generalization | Heavy, not ONNX-friendly |
| Capsule Networks | Rotation invariant | Slow inference |
| Multi-task CNN | Joint detection+classification | Complex training |

## 16.2 Alternative Face Detectors
| Alternative | Pros | Cons |
|---|---|---|
| RetinaFace | More accurate landmarks | Heavier model |
| SCRFD | Very fast | Fewer landmarks |
| YOLOv8-Face | Real-time | No mesh landmarks |
| Dlib HOG+68 | CPU friendly | Poor on angles |

## 16.3 Future Enhancements
- **Temporal analysis:** Analyze face consistency across multiple seconds
- **Multi-camera fusion:** Production stereo depth verification
- **Audio-visual sync:** Correlate lip movement with speech audio
- **Continual learning:** Online adaptation to new deepfake methods
- **AMD NPU deployment:** Full Vitis AI quantization and optimization
- **Multi-face tracking:** Handle meeting/conference scenarios
- **Grad-CAM visualization:** Show which face regions trigger detection

---

# 17. DEPENDENCIES

```
onnxruntime==1.18.0        # ONNX model inference
opencv-python==4.9.0.80    # Video capture, image processing
numpy==1.26.4              # Array math
mediapipe==0.10.14         # Face detection + landmarks
torch==2.2.2               # PyTorch (model loading, CUDA)
torchvision==0.17.2        # Image transforms
timm==1.0.3                # XceptionNet architecture
psutil==5.9.8              # System monitoring
py-cpuinfo==9.0.0          # Hardware detection
PyYAML==6.0.1              # Config loading
Pillow==10.3.0             # Image handling
cryptography               # AES-256-GCM encryption (optional)
scipy                      # Signal processing for rPPG (optional)
```

---

# 18. GLOSSARY

| Term | Definition |
|---|---|
| **EAR** | Eye Aspect Ratio — ratio of eye height to width |
| **rPPG** | Remote Photoplethysmography — detecting pulse from skin color |
| **FFT** | Fast Fourier Transform — frequency domain analysis |
| **Moiré** | Interference pattern from screen pixel grids |
| **Laplacian** | Second derivative operator measuring image sharpness |
| **DBS** | Dynamic Baseline Scaling — adaptive threshold learning |
| **HFER** | High-Frequency Energy Ratio — forensic frequency metric |
| **BAR** | Blocking Artifact Ratio — compression detection metric |
| **SNR** | Signal-to-Noise Ratio — quality of detected signal |
| **GCM** | Galois/Counter Mode — authenticated encryption mode |
| **NCHW** | Batch×Channels×Height×Width tensor format |
| **ONNX** | Open Neural Network Exchange — portable model format |
| **XDNA** | AMD's AI engine architecture for Ryzen AI NPUs |
| **Vitis AI** | AMD's AI inference optimization toolkit |
| **Hysteresis** | Delayed state transitions to prevent flickering |

---

# 19. COMPLETE DEEP-DIVE FOLDER STRUCTURE

Every file and folder in the project, with its exact purpose:

```
SHIELD RYZEN V2 UPDATE 1 INAYAT/
│
│   ══════════════════════════════════════════════════════════
│   ROOT-LEVEL CORE ENGINE FILES (the brain of the prototype)
│   ══════════════════════════════════════════════════════════
│
├── shield_engine.py              [897 lines] CENTRAL ORCHESTRATOR
│   │   The main engine class (ShieldEngine). Contains:
│   │   - Triple-buffer async pipeline (camera→AI→HUD)
│   │   - Thread management (camera_thread, ai_thread)
│   │   - Per-face state machine (PluginAwareStateMachine)
│   │   - Plugin loading and voting aggregation
│   │   - Memory monitoring (GC at >500MB growth)
│   │   - JSONL audit logging per frame
│   │   - Face tracker with IoU-based ID assignment
│   │   - Fake lockout logic (prevents instant de-escalation)
│   │   - FaceResult/EngineResult construction
│   │   THIS IS THE FILE THAT TIES EVERYTHING TOGETHER.
│   │
├── shield_xception.py            [512 lines] NEURAL NETWORK MODEL
│   │   ShieldXception class wrapping timm's XceptionNet.
│   │   - Linear(2048, 2) + Softmax final layer
│   │   - SHA-256 hash verification on model load
│   │   - Key count verification (prevents wrong weights)
│   │   - ModelTamperingError exception class
│   │   - Legacy EAR/texture functions (backward compat)
│   │   - load_model_with_verification() secure loader
│   │
├── shield_face_pipeline.py       [900 lines] FACE DETECTION + PREPROCESSING
│   │   All face detection, alignment, landmark extraction.
│   │   - MediaPipe FaceLandmarker backend (primary)
│   │   - OpenCV DNN SSD backend (fallback)
│   │   - 478→68 landmark index mapping
│   │   - Head pose estimation via cv2.solvePnP()
│   │   - Occlusion scoring from z-depth variance
│   │   - align_and_crop(): resize→BGR2RGB→normalize[-1,1]→NCHW
│   │   - FaceDetection dataclass with all face metadata
│   │   - Landmark confidence estimation
│   │
├── shield_utils_core.py          [1269 lines] ALGORITHMS + DECISION LOGIC
│   │   The mathematical brain. Contains 5 major components:
│   │   A) compute_ear() — EAR with cosine angle compensation
│   │   B) compute_texture_score() — 5-layer screen detection
│   │      ├── Laplacian variance
│   │      ├── Distance-texture physics check
│   │      ├── Moiré pattern detection (FFT radial profile)
│   │      ├── Screen light emission detection
│   │      └── Weak-signal fusion (2+ signals required)
│   │   C) calibrate_device_baseline() — domain adaptation
│   │   D) ConfidenceCalibrator — temperature scaling
│   │   E) DecisionStateMachine — truth table + hysteresis
│   │   Also: BlinkTracker (DBS), SignalSmoother (EMA),
│   │         estimate_distance(), classify_face()
│   │
├── shield_camera.py              [326 lines] CAMERA INPUT + VALIDATION
│   │   ShieldCamera wrapping cv2.VideoCapture.
│   │   - DirectShow backend on Windows
│   │   - Single-frame buffer (latency minimization)
│   │   - Frame validation: shape, dtype, channels, brightness
│   │   - Frame freshness detection (stale frame rejection)
│   │   - Health monitoring: FPS, drop rate, connection status
│   │   - Thread-safe read with proper cleanup
│   │
├── shield_hud.py                 [536 lines] HEADS-UP DISPLAY
│   │   Modern glassmorphism-style fullscreen HUD.
│   │   - Translucent rounded panels
│   │   - Color-coded state badges above faces
│   │   - Right-side dashboard (confidence, EAR, blinks, etc.)
│   │   - Alert banners for FAKE/CRITICAL states
│   │   - Animated pulse ring for VERIFIED state
│   │   - Top bar (logo, LIVE indicator, FPS)
│   │   - Bottom bar (uptime, camera health)
│   │   - Helper: draw_rounded_rect, draw_text, draw_progress_bar
│   │
├── shield_crypto.py              [114 lines] BIOMETRIC ENCRYPTION
│   │   AES-256-GCM in-memory encryption for face data.
│   │   - Ephemeral keys (per session, never stored)
│   │   - Pickle serialization → AES encrypt
│   │   - XOR fallback if cryptography package missing
│   │   - secure_wipe() to dereference keys
│   │   - Singleton pattern for global access
│   │
├── shield_logger.py              [108 lines] AUDIT LOGGING
│   │   ShieldLogger writing JSONL to logs/shield_audit.jsonl.
│   │   - Thread-safe (mutex lock on file writes)
│   │   - ShieldJSONEncoder handles numpy types
│   │   - Levels: AUDIT, WARN, ERROR, SYSTEM
│   │   - log_frame() for per-frame decisions
│   │   - Auto-records system startup/shutdown
│   │
├── shield_plugin.py              [69 lines] PLUGIN INTERFACE
│   │   Abstract base class (ABC) for all plugins.
│   │   - name (str) — unique identifier
│   │   - tier (str) — biometric/forensic/neural/temporal
│   │   - analyze(face, frame) → dict — returns vote
│   │   - release() — optional cleanup
│   │
├── shield_types.py               [32 lines] DATA TYPES
│   │   @dataclass definitions for structured results.
│   │   - FaceResult: bbox, state, confidence, EAR, texture, tiers
│   │   - EngineResult: frame, state, face_results, fps, timing
│   │
├── start_shield.py               [260 lines] MAIN LAUNCHER
│   │   Entry point: python start_shield.py
│   │   - CLI args: --cpu, --source, --windowed, --size, --model
│   │   - Window management (fullscreen/windowed toggle)
│   │   - Keyboard shortcuts (Q/ESC/F/+/-/1-4)
│   │   - Engine selection (ShieldEngine vs RyzenXDNAEngine)
│   │   - Clean shutdown sequence (stop engine → destroy windows)
│   │   - Crash log writing to crash_log.txt
│   │
├── v3_xdna_engine.py             [84 lines] AMD NPU ENGINE
│   │   RyzenXDNAEngine subclass of ShieldEngine.
│   │   - Enforces VitisAI Execution Provider
│   │   - Process priority boost (ABOVE_NORMAL on Windows)
│   │   - NPU status reporting placeholder
│   │   - Backward compat alias: ShieldXDNAEngine
│   │
├── v3_int8_engine.py             [~26K] LEGACY V3 ENGINE
│   │   Earlier iteration of the engine (kept for reference).
│   │   Contains older processing pipeline logic.
│   │
├── v2_onnx.py                    [~8K] LEGACY V2 ONNX ENGINE
│   │   Previous ONNX-only engine version.
│   │
├── export_onnx.py                [76 lines] ONNX EXPORTER
│   │   Converts PyTorch ShieldXception → ONNX format.
│   │   - Dummy input [1,3,299,299], opset 13
│   │   - Dynamic batch axis for flexibility
│   │   - Prerequisite for INT8 quantization
│   │
│   ══════════════════════════════════════════════════════════
│   ROOT-LEVEL CONFIGURATION & DATA FILES
│   ══════════════════════════════════════════════════════════
│
├── config.yaml                   [45 lines] RUNTIME CONFIGURATION
│   │   Tunable parameters: confidence_threshold (0.89),
│   │   blink_threshold (0.24), input_size (299),
│   │   normalization mean/std, eye landmark indices,
│   │   MediaPipe settings, performance throttle.
│   │
├── shield_calibration.json       [~400B] DEVICE CALIBRATION DATA
│   │   Output of calibrate_device_baseline().
│   │   Contains laplacian_mean, ear_baseline, lighting condition.
│   │
├── requirements.txt              [40 lines] PINNED DEPENDENCIES
│   │   All versions locked with == for reproducible builds.
│   │
├── requirements_locked.txt       [~2.4K] FULL LOCKED DEPS
│   │   Extended dependency list with all transitive deps.
│   │
│   ══════════════════════════════════════════════════════════
│   ROOT-LEVEL MODEL FILES (neural network weights)
│   ══════════════════════════════════════════════════════════
│
├── ffpp_c23.pth                  [83.5 MB] PYTORCH WEIGHTS
│   │   FaceForensics++ c23 trained XceptionNet weights.
│   │   Used when ONNX model unavailable. SHA-256 verified.
│   │
├── shield_ryzen_int8.onnx        [21.4 MB] INT8 QUANTIZED ONNX
│   │   Primary inference model. 4x smaller than FP32.
│   │   Optimized for AMD Ryzen AI NPU deployment.
│   │
├── shield_ryzen_v2.onnx          [83.2 MB] FP32 ONNX MODEL
│   │   Full-precision ONNX export (pre-quantization).
│   │
├── shield_ryzen_v2.onnx.data     [84.3 MB] ONNX EXTERNAL DATA
│   │   External weight storage for large ONNX model.
│   │
├── face_landmarker.task          [3.75 MB] MEDIAPIPE MODEL (V1)
│   │   MediaPipe face landmark detection model.
│   │
├── face_landmarker_v2_with_blendshapes.task  [3.75 MB] MEDIAPIPE MODEL (V2)
│   │   Enhanced model with 52 blendshape coefficients.
│   │   Used by shield_face_pipeline.py for blink detection.
│   │
│   ══════════════════════════════════════════════════════════
│   ROOT-LEVEL DOCUMENTATION
│   ══════════════════════════════════════════════════════════
│
├── README.md                     Project overview and quick start
├── GEMINI.md                     AI agent workspace rules (this file)
├── CHANGELOG.md                  Version history and changes
├── LICENSE                       MIT License
├── SECURITY.md                   Security policy
├── CONTRIBUTING.md               Contribution guidelines
├── TRANSFER_GUIDE.md             Guide for porting to new hardware
├── MODEL_CARD.md                 ML model card (dataset, metrics, bias)
├── CLAIMS_VS_EVIDENCE.md         What we claim vs. what we proved
├── DEVELOPMENT_STATE.txt         Current development status
├── VERSION_STAMP.txt             Build version metadata
├── FINAL_AUDIT_RESULT.md         Final project audit score
├── FULL_PROJECT_AUDIT_V2.md      Detailed V2 audit report
├── PROTOTYPE_EXPLAINED.md        ← THIS FILE (complete documentation)
│
│   ══════════════════════════════════════════════════════════
│   ROOT-LEVEL UTILITY/DIAGNOSTIC SCRIPTS
│   ══════════════════════════════════════════════════════════
│
├── shield.py                     [~4K] Minimal standalone shield runner
├── live_webcam_demo.py           [~3.3K] Simple webcam demo without full engine
├── shield_audio.py               [~2.2K] Audio input module (microphone)
├── shield_gradcam.py             [~3.2K] Grad-CAM attention visualization
├── shield_hardware_monitor.py    [~2.6K] CPU/GPU/RAM monitoring
├── validate_system.py            [~2.2K] System compatibility checker
├── verify_model.py               [~17K] Comprehensive model verification
├── realtime_audit.py             [~5.4K] Live audit trail viewer
├── debug_audit.py                [~1.2K] Quick audit log parser
├── quantize_int8.py              [~10K] INT8 static quantization script
├── quantize_int4.py              [~3.6K] INT4 quantization (experimental)
├── quantize_ryzen.py             [~5.5K] AMD Ryzen-specific quantization
├── compile_xmodel.sh             [~1.7K] Vitis AI xmodel compilation
├── start.bat                     [~800B] Windows batch launcher
├── setup_github.ps1              [~1K] PowerShell GitHub repo setup
│
├── _analyze_audit.py             [~2.8K] Post-session audit analysis
├── _audit_to_file.py             [~4.2K] Audit log → readable report
├── _deep_snap.py                 [~4.5K] Deep analysis single-frame snapshot
├── _diag_faces.py                [~1.5K] Face detection diagnostics
├── _diag_texture.py              [~1.8K] Texture analysis diagnostics
├── _ear_test.py                  [~2.6K] EAR calculation test harness
├── _far_debug.py                 [~1.2K] Far-distance face debug
├── _live_audit.py                [~5.8K] Live auditing with console output
├── _snap.py                      [~1.9K] Quick frame capture utility
│
│   ══════════════════════════════════════════════════════════
│   ROOT-LEVEL LOG/OUTPUT FILES (generated at runtime)
│   ══════════════════════════════════════════════════════════
│
├── crash_log.txt                 Last crash traceback
├── audit_log.txt                 Human-readable audit output
├── last_run_audit.txt            Full audit from most recent session
├── final_test.txt                Test suite results
├── final_test_results.txt        Duplicate of test results
├── run_audit_success.txt         Successful audit run log
├── _audit_output.txt             Audit analysis output
├── _audit_result.txt             Parsed audit results
├── network_during_inference.txt  Netstat proof (no network calls)
├── final_evidence_score.json     Competition evidence score
├── model_verification_report.json  Model integrity report
├── quantization_report.json      INT8 quantization metrics
│
│
│   ══════════════════════════════════════════════════════════
│                     SUBDIRECTORIES
│   ══════════════════════════════════════════════════════════
│
├── plugins/                          ← PLUGIN MODULES (9 detection plugins)
│   ├── __init__.py                   Plugin package init (registers all plugins)
│   ├── rppg_heartbeat.py             [172 lines] rPPG HEARTBEAT DETECTION
│   │   Detects blood flow pulse via green channel FFT.
│   │   - Extracts forehead ROI green channel mean per frame
│   │   - Buffers 5 seconds of data (75 samples at 15fps)
│   │   - FFT bandpass 0.7-3.0 Hz (42-180 BPM)
│   │   - SNR > 2.0 with valid BPM range → REAL
│   │   - Uses actual timestamps for accurate sampling rate
│   │
│   ├── challenge_response.py         [163 lines] SIMON SAYS LIVENESS
│   │   Random action challenges to defeat replay attacks.
│   │   - 4 challenges: blink_twice, look_left, smile, raise_eyebrows
│   │   - 5-second timeout per challenge
│   │   - Uses blendshapes for verification
│   │   - Timeout → FAKE verdict (replay can't respond)
│   │
│   ├── stereo_depth.py               [133 lines] DUAL-CAMERA DEPTH
│   │   Detects flat 2D screens vs real 3D faces.
│   │   - Initializes secondary camera (if available)
│   │   - Compares face detection across two views
│   │   - Real nose protrudes → different disparity than ears
│   │   - Flat screen → uniform disparity
│   │
│   ├── skin_reflectance.py           [119 lines] SKIN TEXTURE ANALYSIS
│   │   Distinguishes organic skin from masks/screens.
│   │   - Cheek ROI analysis (avoids T-zone glare)
│   │   - Specular highlight ratio (>25% = unnatural)
│   │   - Mean Sobel gradient (<1.5 = smooth mask)
│   │   - High gradient (>80) = screen Moiré
│   │
│   ├── codec_forensics.py            [118 lines] COMPRESSION DETECTION
│   │   Detects double-compressed video streams.
│   │   - Analyzes 256×256 center crop aligned to 8×8 grid
│   │   - Computes gradient at block boundaries vs internal
│   │   - Blocking Artifact Ratio (BAR) > 1.8 → FAKE
│   │   - Virtual cameras re-encode → higher BAR
│   │
│   ├── frequency_analyzer.py         [117 lines] FFT GAN DETECTION
│   │   Detects GAN/Diffusion generation artifacts.
│   │   - 2D FFT of face crop grayscale
│   │   - Log-magnitude high-freq/low-freq energy ratio
│   │   - HFER < 0.45 = suppressed HF (GAN smoothness)
│   │   - HFER > 0.90 = screen Moiré HF spike
│   │
│   ├── adversarial_detector.py       [113 lines] PATCH DETECTION
│   │   Detects physical adversarial patches/stickers.
│   │   - Sobel gradient magnitude map of face
│   │   - Threshold high-gradient pixels (>250)
│   │   - Dilate + find contours of dense clusters
│   │   - 2+ patches >5% face area with >50% density → FAKE
│   │
│   ├── lip_sync_verifier.py          [105 lines] LIP READING
│   │   Verifies lip shape matches spoken phonemes.
│   │   - 3 phonemes: "O" (pucker), "Ee" (smile), "Ahh" (open)
│   │   - Uses blendshape indices (36=pucker, 25=jaw, 44/45=smile)
│   │   - 5-second timeout, challenge-response style
│   │
│   └── arcface_reid.py              [149 lines] FACE RE-IDENTIFICATION
│       Enterprise identity matching via ArcFace embeddings.
│       - 512-d embeddings from 112×112 aligned crops
│       - Cosine similarity matching against encrypted DB
│       - AES-256 encrypted employee database (local only)
│       - Mock mode if ArcFace ONNX model not present
│
├── models/                           ← MODEL FILES + ML TOOLS
│   ├── shield_ryzen_int8.onnx        [21.4 MB] INT8 quantized model (primary)
│   ├── shield_xception_int4.onnx     [21.4 MB] INT4 quantized (experimental)
│   ├── blazeface.onnx                [426 KB] BlazeFace detector (unused)
│   ├── face_detector.tflite          [230 KB] TFLite face detector (unused)
│   ├── model_signature.sha256        [64B] SHA-256 hash of model weights
│   ├── reference_output.json         [259B] Expected output for test input
│   ├── temperature_scaling_params.npy [136B] Calibrated temperature value
│   ├── attribution_classifier.py     [~2.5K] Deepfake method attribution
│   └── knowledge_distillation.py     [~4.4K] Teacher→student model compression
│
├── config/                           ← RUNTIME CONFIGURATION
│   └── decision_thresholds.yaml      [680B] Optimized decision thresholds
│       │   Generated by evaluation/threshold_optimization.py
│       │   Contains per-tier threshold overrides
│
├── plugins/                          (described above)
│
├── tests/                            ← TEST SUITE (16 test files)
│   ├── __init__.py                   Test package init
│   ├── test_model.py                 [15K] XceptionNet loading, inference, hash
│   ├── test_utils.py                 [19K] EAR, texture, calibration, state machine
│   ├── test_face_pipeline.py         [17K] Detection, landmarks, normalization
│   ├── test_engine.py                [~8K] Engine init, threading, processing
│   ├── test_camera.py                [~9K] Camera capture, validation, health
│   ├── test_biometric_plugins.py     [~7K] rPPG, challenge-response, depth
│   ├── test_forensic_plugins.py      [~8K] Frequency, codec, adversarial, lip
│   ├── test_hud.py                   [~4K] HUD rendering, color palette
│   ├── test_advanced_detection.py    [~5K] Edge cases, angle compensation
│   ├── test_amd_hardware.py          [~5K] AMD NPU compatibility checks
│   ├── test_amd_npu.py               [~4K] XDNA engine, Vitis AI EP
│   ├── test_enterprise.py            [~5K] ArcFace, encryption, audit
│   ├── test_integration_final.py     [~4K] End-to-end pipeline test
│   ├── test_quantization.py          [~4K] INT8/INT4 accuracy verification
│   ├── skipped_integration_full.py   [~9K] Skipped heavy integration tests
│   ├── fixtures/                     Test fixture data (synthetic faces)
│   └── temp_final/                   Temporary test outputs
│
├── scripts/                          ← BUILD + MAINTENANCE SCRIPTS
│   ├── emergency_diagnostic.py       [~40K] Comprehensive system diagnostics
│   ├── export_verified_onnx.py       [~2.7K] Verified ONNX export pipeline
│   ├── generate_calibration_v2.py    [~9K] Multi-frame calibration generator
│   ├── generate_evidence_package.ps1 [~1.9K] Evidence package PowerShell script
│   ├── generate_test_fixtures.py     [~5.8K] Synthetic test data generator
│   ├── inspect_blazeface.py          [~420B] BlazeFace model inspector
│   ├── setup_blazeface.py            [~2.2K] BlazeFace setup utility
│   ├── verify_integrity.py           [~1.6K] File integrity checker
│   └── verify_normalization.py       [~10K] Normalization correctness validator
│
├── benchmarks/                       ← PERFORMANCE BENCHMARKS
│   ├── baseline.py                   [~16K] Comprehensive baseline benchmark
│   ├── benchmark_accuracy.py         [~5K] Accuracy on test datasets
│   ├── benchmark_fps.py              [~6K] FPS measurement across configs
│   ├── benchmark_plugins.py          [~4K] Per-plugin latency measurement
│   ├── benchmark_power.py            [~2.2K] Power consumption estimation
│   ├── benchmark_threshold.py        [~3K] Threshold sensitivity analysis
│   ├── benchmark_blazeface.py        [~1.4K] BlazeFace speed test
│   ├── edge_case_test_suite.py       [~6K] Edge case stress testing
│   ├── final_performance_report.py   [~3.5K] Aggregated report generator
│   ├── run_netstat_proof.py          [~1.6K] Network isolation proof
│   ├── accuracy_report.json          Accuracy benchmark results
│   ├── fps_report.json               FPS benchmark results
│   ├── power_report.json             Power benchmark results
│   ├── threshold_report.json         Threshold analysis results
│   └── roc_curves/                   ROC curve data and plots
│
├── security/                         ← SECURITY TESTING
│   ├── adversarial_test_suite.py     [~14K] Adversarial attack simulations
│   ├── audit_trail.py                [~4.6K] Audit trail integrity checker
│   ├── ftpm_wrapper.py               [~2.1K] TPM key storage wrapper
│   ├── verify_integrity.py           [~3.6K] Full integrity verification
│   ├── adversarial_robustness.json   Robustness test results (detailed)
│   └── adversarial_robustness_report.json  Summary robustness report
│
├── evaluation/                       ← ML EVALUATION
│   ├── auc_validation.py             [~12K] AUC/ROC curve validation
│   └── threshold_optimization.py     [~12K] Optimal threshold search
│
├── evidence_package/                 ← COMPETITION EVIDENCE (AMD Slingshot)
│   ├── FINAL_AUDIT_RESULT.md         Final audit score document
│   ├── accuracy_report.json          Accuracy evidence
│   ├── adversarial_robustness_report.json  Security evidence
│   ├── final_evidence_score.json     Combined evidence score
│   ├── final_test_results.txt        Test pass/fail evidence
│   ├── fps_report.json               Performance evidence
│   ├── model_signature.sha256        Model integrity evidence
│   ├── model_verification_report.json  Model verification evidence
│   ├── network_during_inference.txt  Privacy evidence (no network)
│   ├── power_report.json             Power efficiency evidence
│   ├── quantization_report.json      Quantization evidence
│   ├── requirements_locked.txt       Dependency reproducibility evidence
│   ├── test_results.txt              Unit test evidence
│   ├── threshold_report.json         Threshold calibration evidence
│   └── roc_curves/                   ROC curve evidence plots
│
├── docs/                             ← DOCUMENTATION
│   ├── architecture.md               [~4K] System architecture reference
│   ├── DEVELOPMENT_LOG.md            [~23K] Full development timeline
│   ├── PROTOTYPE_EXPLAINED.md        [~25K] Deep-dive documentation
│   ├── PROTOTYPE_AUDIT_REPORT.md     [~2K] Audit report
│   ├── COMPLIANCE.md                 [~3K] GDPR/CCPA compliance guide
│   ├── THREAT_MODEL.md               [~3K] Security threat analysis
│   └── VALIDATION_REPORT.md          [~2K] System validation results
│
├── shield_utils/                     ← UTILITY SUBPACKAGE
│   ├── __init__.py                   [~2K] Package init (re-exports utilities)
│   ├── blazeface_detector.py         [~6.5K] BlazeFace ONNX detector
│   ├── calibrated_decision.py        [~1.6K] Calibrated decision helper
│   └── occlusion_detector.py         [~6K] Face occlusion analysis
│
├── performance/                      ← PERFORMANCE OPTIMIZATION
│   ├── preprocessing_worker.py       [~3.9K] Background preprocessing thread
│   └── zero_copy_buffer.py           [~1.3K] Zero-copy frame buffer
│
├── shield_audio_module/              ← AUDIO PROCESSING
│   └── lip_sync_verifier.py          [~1.8K] Audio-visual lip sync
│
├── shield_liveness/                  ← LIVENESS DETECTION
│   └── challenge_response.py         [~3K] Earlier challenge-response version
│
├── shield_temporal/                  ← TEMPORAL ANALYSIS
│   └── temporal_consistency.py       [~4.4K] Frame-to-frame consistency
│
├── shield_frequency/                 ← FREQUENCY ANALYSIS
│   └── frequency_analyzer.py         [~3.8K] Earlier frequency analyzer version
│
├── camera/                           ← CAMERA UTILITIES
│   └── amd_direct_capture.py         [~2.1K] AMD DirectCapture API wrapper
│
├── logs/                             ← RUNTIME LOGS (generated)
│   ├── shield_audit.jsonl            JSONL audit trail (per-frame)
│   └── texture_debug.log             Texture analysis debug output
│
├── PRESENTATION DATA/                ← COMPETITION PRESENTATION ASSETS
│   ├── IMAGES/                       Presentation screenshots/diagrams
│   ├── MINDMAP/                      Architecture mind maps
│   ├── PODCAST/                      Audio presentation files
│   ├── PPT/                          PowerPoint slides
│   └── VIDEOS/                       Demo video recordings
│
├── data/                             ← TRAINING/TEST DATA (empty/gitignored)
├── diagnostics/                      ← DIAGNOSTIC OUTPUT (empty/generated)
│
├── .git/                             Git repository
├── .gitignore                        Git ignore rules
├── .venv/                            Python virtual environment
└── __pycache__/                      Python bytecode cache
```

## 19.1 File Count Summary

| Category | Files | Total Size |
|---|---|---|
| **Core Engine** (root .py) | 14 files | ~190 KB |
| **Plugins** | 9 files + __init__ | ~48 KB |
| **Models** (weights) | 6 model files | ~213 MB |
| **Tests** | 16 test files | ~120 KB |
| **Scripts** | 9 utility scripts | ~74 KB |
| **Benchmarks** | 10 scripts + 5 reports | ~52 KB |
| **Security** | 4 scripts + 2 reports | ~25 KB |
| **Evaluation** | 2 scripts | ~24 KB |
| **Documentation** | 15 markdown files | ~80 KB |
| **Evidence Package** | 15 evidence files | ~70 KB |
| **Subpackages** | 8 files across 6 packages | ~25 KB |
| **Config** | 3 YAML/JSON files | ~3.4 KB |
| **TOTAL** | ~100+ source files | ~213 MB (models dominate) |

## 19.2 Which Files Actually Run During Inference?

When you execute `python start_shield.py --cpu`, these files are loaded:

```
STARTUP CHAIN:
  start_shield.py
    → v3_xdna_engine.py (or shield_engine.py directly with --cpu)
      → shield_engine.py ← MAIN ORCHESTRATOR
        → shield_camera.py
        → shield_face_pipeline.py
        → shield_xception.py (model loading)
        → shield_utils_core.py (all algorithms)
        → shield_crypto.py
        → shield_logger.py
        → shield_types.py (dataclasses)
        → config.yaml (configuration)
        → plugins/__init__.py
          → plugins/rppg_heartbeat.py
          → plugins/challenge_response.py
          → plugins/stereo_depth.py
          → plugins/skin_reflectance.py
          → plugins/codec_forensics.py
          → plugins/frequency_analyzer.py
          → plugins/adversarial_detector.py
          → plugins/lip_sync_verifier.py
          → plugins/arcface_reid.py
      → shield_hud.py ← HUD RENDERING

MODEL FILES LOADED:
  → face_landmarker_v2_with_blendshapes.task (MediaPipe)
  → shield_ryzen_int8.onnx (or ffpp_c23.pth fallback)

RUNTIME FILES CREATED/UPDATED:
  → logs/shield_audit.jsonl (audit trail)
  → logs/texture_debug.log (debug output)
  → shield_calibration.json (if first run)
```

**Files NOT loaded during inference:** All test files, benchmarks, scripts, evaluation, security tests, evidence package, documentation, legacy engines, and presentation data. These are development/competition support files only.

---

*Documentation generated for AMD Slingshot 2026 competition.*
*Developer: Inayat Hussain*
*Shield-Ryzen V2 — UPDATE 1*
*Last updated: 2026-02-22*


## Document Approval

| Role | Name | Date | Status |
|---|---|---|---|
| Developer | Inayat Hussain | July 2025 | ✅ Authored |
| Competition | AMD Slingshot 2026 | — | Submitted |

---

*This PRD documents a vibe coding prototype developed for the AMD Slingshot 2026 competition. All specifications reflect the current prototype state and are subject to iteration.*

*Shield-Ryzen V2 — PRD v1.0*